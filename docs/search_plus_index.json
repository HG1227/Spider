{"./":{"url":"./","title":"Introduction","keywords":"","body":"Introduction Update time： 2020-05-26 08:41 "},"Chapter1/":{"url":"Chapter1/","title":"基础","keywords":"","body":"基础 Update time： 2020-05-26 09:23 "},"Chapter1/requests库.html":{"url":"Chapter1/requests库.html","title":"requests库","keywords":"","body":"requests库 安装 使用pip进行安装 要安装 requests，最方便快捷的方法是使用 pip 进行安装。 pip install requests 如果还没有安装pip，这个链接 Properly Installing Python 详细介绍了在各种平台下如何安装 python 以及 setuptools，pip，virtualenv 等常用的 python 工具，可以参考其中的步骤来进行安装。 get或post请求 # 不带可选参数的get请求 >>> r = requests.get(url='https://github.com/timeline.json') # 不带可选参数的post请求 >>> r = requests.post(url=\"http://httpbin.org/post\") get\\post参数说明 http 请求 get 与 post 是最常用的，url 为必选参数，常用常用参数有params、data、json、files、timeout、headers、cookies；其他基本用不到的有verify，cert，auth，allow_redirects，proxies，hooks，stream。 下面列表对具体的参数说明： 重点在 params、data、json、files、timeout，其次headers、cookies，其他略过就可以。 get请求文本与二进制数据内容(图片) get请求是最简单的、发送的数据量比较小。 示例2.1: 带多个参数的请求，返回文本数据 import requests # 带参数的GET请求,timeout请求超时时间 params = {'key1': 'python', 'key2': 'java'} r = requests.get(url='http://httpbin.org/get', params=params, timeout=3) # 注意观察url地址，它已经将参数拼接起来 print('URL地址：', r.url) # 响应状态码，成功返回200，失败40x或50x print('请求状态码：', r.status_code) print('header信息:', r.headers) print('cookie信息：', r.cookies) print('响应的数据：', r.text) # 如响应是json数据 ，可以使用 r.json()自动转换为dict print('响应json数据', r.json()) 示例2.2：get 返回二进制数据，如图片。 from PIL import Image from io import BytesIO import requests # 请求获取图片并保存 r = requests.get('https://pic3.zhimg.com/247d9814fec770e2c85cc858525208b2_is.jpg') i = Image.open(BytesIO(r.content)) # i.show() # 查看图片 # 将图片保存 with open('img.jpg', 'wb') as fd: for chunk in r.iter_content(): fd.write(chunk) post请求，上传表单，文本，文件\\图片 post 请求比 get 复杂，请求的数据有多种多样，有表单(form-data)，文本(json\\xml等)，文件流（图片\\文件）等。 表单形式提交的post请求，只需要将数据传递给post()方法的data参数。见示例3.1. json文本形式提交的post请求，一种方式是将json数据dumps后传递给data参数，另一种方式就是直接将json数据传递给post()方法的json参数。见示例3.2. 单个文件提交的post请求，将文件流给post()方法的files参数。见示例3.3。 多个文件提交的post请求，将文件设到一个元组的列表中，其中元组结构为 (form_field_name, file_info)；然后将数据传递给post()方法的files。见示例3.4 示例3.1：post 表单请求 import requests, json # 带参数表单类型post请求 data={'custname': 'woodman','custtel':'13012345678','custemail':'woodman@11.com', 'size':'small'} r = requests.post('http://httpbin.org/post', data=data) print('响应数据：', r.text) 示例3.2：post json请求 # json数据请求 url = 'https://api.github.com/some/endpoint' payload = {'some': 'data'} # 可以使用json.dumps(dict) 对编码进行编译 r = requests.post(url, data=json.dumps(payload)) print('响应数据：', r.text) # 可以直接使用json参数传递json数据 r = requests.post(url, json=payload) print('响应数据：', r.text) 示例3.3：post提交单个文件 # 上传单个文件 url = 'http://httpbin.org/post' # 注意文件打开的模式，使用二进制模式不容易发生错误 files = {'file': open('report.txt', 'rb')} # 也可以显式地设置文件名，文件类型和请求头 # files = {'file': ('report.xls', open('report.xls', 'rb'), 'application/vnd.ms-excel', {'Expires': '0'})} r = requests.post(url, files=files) r.encoding = 'utf-8' print(r.text) 注意：文件的上传使用二进制打开不容易报错。 示例3.4：上传多个文件 url = 'http://httpbin.org/post' multiple_files = [ ('images', ('foo.png', open('foo.png', 'rb'), 'image/png')), ('images', ('bar.png', open('bar.png', 'rb'), 'image/png'))] r = requests.post(url, files=multiple_files) print(r.text) get与post请求的header与cookie管理 获取 get 与 post 请求响应的 header 与 cookie 分别使用 r.headers 与r.cookies 。如果提交请求数据是对 header 与 cookie 有修改，需要在get()与post()方法中加入 headers 或 cookies 参数，它们值的类型都是字典 示例4.1：定制请求头header import requests url = 'https://api.github.com/some/endpoint' headers = {'user-agent': 'my-app/0.0.1'} r = requests.get(url, headers=headers) print(r.headers) # 获取响应数据的header信息 注意：requests自带headers管理，一般情况下不需要设置header信息。Requests 不会基于定制 header 的具体情况改变自己的行为。只不过在最后的请求中，所有的 header 信息都会被传递进去。 示例4.2：定制cookie信息 # 直接以字典型时传递cookie url = 'http://httpbin.org/cookies' cookies = {\"cookies_are\":'working'} r = requests.get(url, cookies=cookies) # 获取响应的cookie信息，返回结果是RequestsCookieJar对象 print(r.cookies) print(r.text) session与cookie存储 如果你向同一主机发送多个请求，每个请求对象让你能够跨请求保持session和cookie信息，这时我们要使用到requests的Session()来保持回话请求的cookie和session与服务器的相一致。 import requests url = \"http://www.renren.com/PLogin.do\" data = {\"email\":\"970138074@qq.com\",'password':\"pythonspider\"} headers = { 'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36\" } # 登录 session = requests.session() session.post(url,data=data,headers=headers) # 访问大鹏个人中心 resp = session.get('http://www.renren.com/880151247/profile') print(resp.text) requests请求返回对象Response的常用方法 requests.get(url) 与 requests.post(url) 的返回对象为Response 类对象。 Response响应类常用属性与方法： Response.url 请求url，[见示例2.1] Response.status_code 响应状态码，[见示例2.1] Response.text 获取响应内容，[见示例2.1] Response.content 以字节形式获取响应提，多用于非文本请求，[见示例2.2] Response.json() 活动响应的JSON内容，[见示例2.1] Response.ok 请求是否成功，status_code 参考 python3之requests Update time： 2020-05-26 11:47 "},"Chapter1/XPath语法和lxml模块.html":{"url":"Chapter1/XPath语法和lxml模块.html","title":"XPath语法和lxml模块","keywords":"","body":"XPath语法和lxml模块 什么是XPath？ xpath（XML Path Language）是一门在XML和HTML文档中查找信息的语言，可用来在XML和HTML文档中对元素和属性进行遍历。 XPath语法 选取节点： XPath 使用路径表达式来选取 XML 文档中的节点或者节点集。这些路径表达式和我们在常规的电脑文件系统中看到的表达式非常相似。 表达式 描述 示例 结果 nodename 选取此节点的所有子节点 bookstore 选取bookstore下所有的子节点 / 如果是在最前面，代表从根节点选取。否则选择某节点下的某个节点 /bookstore 选取根元素下所有的bookstore节点 // 从全局节点中选择节点，随便在哪个位置 //book 从全局节点中找到所有的book节点 @ 选取某个节点的属性 //book[@price] 选择所有拥有price属性的book节点 . 当前节点 ./a 选取当前节点下的a标签 谓语： 谓语用来查找某个特定的节点或者包含某个指定的值的节点，被嵌在方括号中。 在下面的表格中，我们列出了带有谓语的一些路径表达式，以及表达式的结果： 通配符 *表示通配符 通配符 描述 示例 结果 * 匹配任意节点 /bookstore/* 选取bookstore下的所有子元素。 @* 匹配节点中的任何属性 //book[@*] 选取所有带有属性的book元素。 选取多个路径： 通过在路径表达式中使用 “|” 运算符，可以选取若干个路径。 //bookstore/book | //book/title # 选取所有book元素以及book元素下所有的title元素 lxml库 lxml 是 一个HTML/XML的解析器，主要的功能是如何解析和提取 HTML/XML 数据。 lxml和正则一样，也是用 C 实现的，是一款高性能的 Python HTML/XML 解析器，我们可以利用之前学习的XPath语法，来快速的定位特定元素以及节点信息。 基本使用： 解析HTML代码，并且在解析HTML代码的时候，如果HTML代码不规范，他会自动的进行补全。示例代码如下： # 使用 lxml 的 etree 库 from lxml import etree text = ''' first item second item third item fourth item fifth item # 注意，此处缺少一个 闭合标签 ''' #利用etree.HTML，将字符串解析为HTML文档 html = etree.HTML(text) # 按字符串序列化HTML文档 result = etree.tostring(html) print(result) 输入结果如下： first item second item third item fourth item fifth item 可以看到。lxml会自动修改HTML代码。例子中不仅补全了li标签，还添加了body，html标签。 从文件中读取html代码 除了直接使用字符串进行解析，lxml还支持从文件中读取内容。我们新建一个hello.html文件： first item second item third item fourth item fifth item 然后利用 etree.parse()方法来读取文件。示例代码如下： from lxml import etree # 读取外部文件 hello.html html = etree.parse('hello.html') result = etree.tostring(html, pretty_print=True) print(result) 在lxml中使用XPath语法： 读取文本解析节点 from lxml import etree text=''' 第一个 second item a属性 ''' html=etree.HTML(text) #初始化生成一个XPath解析对象 result=etree.tostring(html,encoding='utf-8') #解析对象输出代码 print(type(html)) print(type(result)) print(result.decode('utf-8')) #etree会修复HTML文本节点 第一个 second item a属性 读取HTML文件进行解析 from lxml import etree html=etree.parse('test.html',etree.HTMLParser()) #指定解析器HTMLParser会根据文件修复HTML文件中缺失的如声明信息 result=etree.tostring(html) #解析成字节 #result=etree.tostringlist(html) #解析成列表 print(type(html)) print(type(result)) print(result) # b'\\n \\n \\n first item \\n second item \\n third item \\n fourth item \\n fifth item \\n \\n \\n' 获取所有节点 返回一个列表每个元素都是Element类型，所有节点都包含在其中 from lxml import etree html=etree.parse('test',etree.HTMLParser()) result=html.xpath('//*') #//代表获取子孙节点，*代表获取所有 print(type(html)) print(type(result)) print(result) # [, , , , , , , , , , , , , ] 如要获取li节点，可以使用//后面加上节点名称，然后调用xpath()方法 html.xpath('//li') #获取所有子孙节点的li节点 获取子节点 通过/或者//即可查找元素的子节点或者子孙节点，如果想选择li节点的所有直接a节点，可以这样使用 result=html.xpath('//li/a') #通过追加/a选择所有li节点的所有直接a节点， # 因为//li用于选中所有li节点，/a用于选中li节点的所有直接子节点a 属性匹配 在选取的时候，我们还可以用@符号进行属性过滤。比如，这里如果要选取class为item-1的li节点，可以这样实现: from lxml import etree from lxml.etree import HTMLParser text=''' 第一个 second item ''' html=etree.HTML(text,etree.HTMLParser()) result=html.xpath('//li[@class=\"item-1\"]') print(result) 文本获取 我们用XPath中的text()方法获取节点中的文本 from lxml import etree text=''' 第一个 second item ''' html=etree.HTML(text,etree.HTMLParser()) result=html.xpath('//li[@class=\"item-1\"]/a/text()') #获取a节点下的内容 result1=html.xpath('//li[@class=\"item-1\"]//text()') #获取li下所有子孙节点的内容 print(result) print(result1) 属性获取 使用@符号即可获取节点的属性，如下：获取所有li节点下所有a节点的href属性 result=html.xpath('//li/a/@href') #获取a的href属性 result=html.xpath('//li//@href') #获取所有li子孙节点的href属性 属性多值匹配 如果某个属性的值有多个时，我们可以使用contains()函数来获取 from lxml import etree text1=''' 第一个 second item ''' html=etree.HTML(text1,etree.HTMLParser()) result=html.xpath('//li[@class=\"aaa\"]/a/text()') result1=html.xpath('//li[contains(@class,\"aaa\")]/a/text()') print(result) print(result1) #通过第一种方法没有取到值，通过contains（）就能精确匹配到节点了 [] ['第一个'] 多属性匹配 另外我们还可能遇到一种情况，那就是根据多个属性确定一个节点，这时就需要同时匹配多个属性，此时可用运用and运算符来连接使用： from lxml import etree text1=''' 第一个 second item ''' html=etree.HTML(text1,etree.HTMLParser()) result=html.xpath('//li[@class=\"aaa\" and @name=\"fore\"]/a/text()') result1=html.xpath('//li[contains(@class,\"aaa\") and @name=\"fore\"]/a/text()') print(result) print(result1) # ['second item'] ['second item'] 按序选择 有时候，我们在选择的时候某些属性可能同时匹配多个节点，但我们只想要其中的某个节点，如第二个节点或者最后一个节点，这时可以利用中括号引入索引的方法获取特定次序的节点： from lxml import etree text1=''' 第一个 第二个 第三个 第四个 ''' html=etree.HTML(text1,etree.HTMLParser()) result=html.xpath('//li[contains(@class,\"aaa\")]/a/text()') #获取所有li节点下a节点的内容 result1=html.xpath('//li[1][contains(@class,\"aaa\")]/a/text()') #获取第一个 result2=html.xpath('//li[last()][contains(@class,\"aaa\")]/a/text()') #获取最后一个 result3=html.xpath('//li[position()>2 and position() 这里使用了last()、position()函数，在XPath中，提供了100多个函数，包括存取、数值、字符串、逻辑、节点、序列等处理功能，它们的具体作用可参考：http://www.w3school.com.cn/xpath/xpath_functions.asp lxml结合xpath注意事项： 使用xpath语法。应该使用Element.xpath方法。来执行xpath的选择。示例代码如下 trs = html.xpath(\"//tr[position()>1]\") xpath函数返回来的永远是一个列表。 获取某个标签的属性： href = html.xpath(\"//a/@href\") # 获取a标签的href属性对应的值 若再某个标签下 也可以直接用get()函数 html = etree.HTML(text) imgs = html.xpath(\"//div[@class='page-content text-center']//img[@class!='gif']\") for img in imgs: img_url = img.get('data-original') alt = img.get('alt') 获取文本，是通过xpath中的text()函数。示例代码如下： address = tr.xpath(\"./td[4]/text()\")[0] 在某个标签下，再执行xpath函数，获取这个标签下的子孙元素，那么应该在斜杠之前加一个点. ，代表是在当前元素下获取。示例代码如下： address = tr.xpath(\"./td[4]/text()\")[0] # address = tr.xpath(\"./td[4]//text()\")[0] 子孙标签的所有文本 需要注意的知识点： / 和// 的区别：/ 代表只获取直接子节点。// 获取子孙节点。一般// 用得比较多。当然也要视情况而定。 contains：有时候某个属性中包含了多个值，那么可以使用contains函数。示例代码如下：//div[contains(@class,'job_detail')] 谓词中的下标是从1开始的，不是从0开始的。 练习 #encoding: utf-8 from lxml import etree # 1. 获取所有tr标签 # 2. 获取第2个tr标签 # 3. 获取所有class等于even的tr标签 # 4. 获取所有a标签的href属性 # 5. 获取所有的职位信息（纯文本） parser = etree.HTMLParser(encoding='utf-8') html = etree.parse(\"tencent.html\",parser=parser) # 1. 获取所有tr标签 # //tr # xpath函数返回的是一个列表 # trs = html.xpath(\"//tr\") # for tr in trs: # print(etree.tostring(tr,encoding='utf-8').decode(\"utf-8\")) # 2. 获取第2个tr标签 # tr = html.xpath(\"//tr[2]\")[0] # print(etree.tostring(tr,encoding='utf-8').decode(\"utf-8\")) # 3. 获取所有class等于even的tr标签 # trs = html.xpath(\"//tr[@class='even']\") # for tr in trs: # print(etree.tostring(tr,encoding='utf-8').decode(\"utf-8\")) # 4. 获取所有a标签的href属性 # aList = html.xpath(\"//a/@href\") # for a in aList: # print(\"http://hr.tencent.com/\"+a) # 5. 获取所有的职位信息（纯文本） trs = html.xpath(\"//tr[position()>1]\") positions = [] for tr in trs: # 在某个标签下，再执行xpath函数，获取这个标签下的子孙元素 # 那么应该在//之前加一个点，代表是在当前元素下获取 href = tr.xpath(\".//a/@href\")[0] fullurl = 'http://hr.tencent.com/' + href title = tr.xpath(\"./td[1]//text()\")[0] category = tr.xpath(\"./td[2]/text()\")[0] nums = tr.xpath(\"./td[3]/text()\")[0] address = tr.xpath(\"./td[4]/text()\")[0] pubtime = tr.xpath(\"./td[5]/text()\")[0] position = { 'url': fullurl, 'title': title, 'category': category, 'nums': nums, 'address': address, 'pubtime': pubtime } positions.append(position) print(positions) 案例应用：抓取TIOBE指数前20名排行开发语言 #!/usr/bin/env python #coding:utf-8 import requests from requests.exceptions import RequestException from lxml import etree from lxml.etree import ParseError import json def one_to_page(html): headers={ 'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.62 Safari/537.36' } try: response=requests.get(html,headers=headers) body=response.text #获取网页内容 except RequestException as e: print('request is error!',e) try: html=etree.HTML(body,etree.HTMLParser()) #解析HTML文本内容 result=html.xpath('//table[contains(@class,\"table-top20\")]/tbody/tr//text()') #获取列表数据 pos = 0 for i in range(20): if i == 0: yield result[i:5] else: yield result[pos:pos+5] #返回排名生成器数据 pos+=5 except ParseError as e: print(e.position) def write_file(data): #将数据重新组合成字典写入文件并输出 for i in data: sul={ '2018年6月排行':i[0], '2017年6排行':i[1], '开发语言':i[2], '评级':i[3], '变化率':i[4] } with open('test.txt','a',encoding='utf-8') as f: f.write(json.dumps(sul,ensure_ascii=False) + '\\n') #必须格式化数据 f.close() print(sul) return None def main(): url='https://www.tiobe.com/tiobe-index/' data=one_to_page(url) revaule=write_file(data) if revaule == None: print('ok') if __name__ == '__main__': main() # {'2018年6月排行': '1', '2017年6排行': '1', '开发语言': 'Java', '评级': '15.368%', '变化率': '+0.88%'} {'2018年6月排行': '2', '2017年6排行': '2', '开发语言': 'C', '评级': '14.936%', '变化率': '+8.09%'} {'2018年6月排行': '3', '2017年6排行': '3', '开发语言': 'C++', '评级': '8.337%', '变化率': '+2.61%'} {'2018年6月排行': '4', '2017年6排行': '4', '开发语言': 'Python', '评级': '5.761%', '变化率': '+1.43%'} {'2018年6月排行': '5', '2017年6排行': '5', '开发语言': 'C#', '评级': '4.314%', '变化率': '+0.78%'} {'2018年6月排行': '6', '2017年6排行': '6', '开发语言': 'Visual Basic .NET', '评级': '3.762%', '变化率': '+0.65%'} {'2018年6月排行': '7', '2017年6排行': '8', '开发语言': 'PHP', '评级': '2.881%', '变化率': '+0.11%'} {'2018年6月排行': '8', '2017年6排行': '7', '开发语言': 'JavaScript', '评级': '2.495%', '变化率': '-0.53%'} {'2018年6月排行': '9', '2017年6排行': '-', '开发语言': 'SQL', '评级': '2.339%', '变化率': '+2.34%'} {'2018年6月排行': '10', '2017年6排行': '14', '开发语言': 'R', '评级': '1.452%', '变化率': '-0.70%'} {'2018年6月排行': '11', '2017年6排行': '11', '开发语言': 'Ruby', '评级': '1.253%', '变化率': '-0.97%'} {'2018年6月排行': '12', '2017年6排行': '18', '开发语言': 'Objective-C', '评级': '1.181%', '变化率': '-0.78%'} {'2018年6月排行': '13', '2017年6排行': '16', '开发语言': 'Visual Basic', '评级': '1.154%', '变化率': '-0.86%'} {'2018年6月排行': '14', '2017年6排行': '9', '开发语言': 'Perl', '评级': '1.147%', '变化率': '-1.16%'} {'2018年6月排行': '15', '2017年6排行': '12', '开发语言': 'Swift', '评级': '1.145%', '变化率': '-1.06%'} {'2018年6月排行': '16', '2017年6排行': '10', '开发语言': 'Assembly language', '评级': '0.915%', '变化率': '-1.34%'} {'2018年6月排行': '17', '2017年6排行': '17', '开发语言': 'MATLAB', '评级': '0.894%', '变化率': '-1.10%'} {'2018年6月排行': '18', '2017年6排行': '15', '开发语言': 'Go', '评级': '0.879%', '变化率': '-1.17%'} {'2018年6月排行': '19', '2017年6排行': '13', '开发语言': 'Delphi/Object Pascal', '评级': '0.875%', '变化率': '-1.28%'} {'2018年6月排行': '20', '2017年6排行': '20', '开发语言': 'PL/SQL', '评级': '0.848%', '变化率': '-0.72%'} 使用requests和xpath爬取电影天堂 import requests from lxml import etree BASE_DOMAIN = 'http://www.dytt8.net' HEADERS = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36', 'Referer': 'http://www.dytt8.net/html/gndy/dyzz/list_23_2.html' } def spider(): url = 'http://www.dytt8.net/html/gndy/dyzz/list_23_1.html' resp = requests.get(url,headers=HEADERS) # resp.content：经过编码后的字符串 # resp.text：没有经过编码，也就是unicode字符串 # text：相当于是网页中的源代码了 text = resp.content.decode('gbk') # tree：经过lxml解析后的一个对象，以后使用这个对象的xpath方法，就可以 # 提取一些想要的数据了 tree = etree.HTML(text) # xpath/beautifulsou4 all_a = tree.xpath(\"//div[@class='co_content8']//a\") for a in all_a: title = a.xpath(\"text()\")[0] href = a.xpath(\"@href\")[0] if href.startswith('/'): detail_url = BASE_DOMAIN + href crawl_detail(detail_url) break def crawl_detail(url): resp = requests.get(url,headers=HEADERS) text = resp.content.decode('gbk') tree = etree.HTML(text) create_time = tree.xpath(\"//div[@class='co_content8']/ul/text()\")[0].strip() imgs = tree.xpath(\"//div[@id='Zoom']//img/@src\") # 电影海报 cover = imgs[0] # 电影截图 screenshoot = imgs[1] # 获取span标签下所有的文本 infos = tree.xpath(\"//div[@id='Zoom']//text()\") for index,info in enumerate(infos): if info.startswith(\"◎年　　代\"): year = info.replace(\"◎年　　代\",\"\").strip() if info.startswith(\"◎豆瓣评分\"): douban_rating = info.replace(\"◎豆瓣评分\",'').strip() print(douban_rating) if info.startswith(\"◎主　　演\"): # 从当前位置，一直往下面遍历 actors = [info] for x in range(index+1,len(infos)): actor = infos[x] if actor.startswith(\"◎\"): break actors.append(actor.strip()) print(\",\".join(actors)) if __name__ == '__main__': spider() 参考 XPath语法详解 XPath-语法大全 XPath的更多用法参考：http://www.w3school.com.cn/xpath/index.asp python lxml库的更多用法参考：http://lxml.de/ python3解析库lxml Update time： 2020-05-26 11:45 "},"Chapter1/正则表达式匹配.html":{"url":"Chapter1/正则表达式匹配.html","title":"正则表达式匹配","keywords":"","body":"正则表达式匹配 正则表达式抓取网络数据常见方法 HTML语言是采用标签对的形式来编写网站的，包括起始标签和结束标签，比如、、等。下面讲解抓取标签对之间的文本内容。 抓取title标签间的内容 首先爬取网页的标题，采用的正则表达式为(.*?)，爬取百度标题代码如下： # coding=utf-8 import re import urllib url = \"http://www.baidu.com/\" content = urllib.urlopen(url).read() title = re.findall(r'(.*?)', content) print title[0] # 百度一下，你就知道 抓取超链接标签间的内容 在HTML中，用于标识超链接，获取完整的超链接和超链接和之间的内容 # coding=utf-8 import re import urllib url = \"http://www.baidu.com/\" content = urllib.urlopen(url).read() #获取完整超链接 res = r\"\" urls = re.findall(res, content) for u in urls: print unicode(u,'utf-8') #获取超链接和之间内容 res = r'(.*?)' texts = re.findall(res, content, re.S|re.M) for t in texts: print unicode(t,'utf-8') 输出结果部分内容如下所示，这里如果直接输出print u或print t可能会乱码，需要调用函数unicode(u,'utf-8')进行转码 #获取完整超链接 新闻 hao123 地图 视频 ... #获取超链接和之间内容 新闻 hao123 地图 视频 ... 抓取tr\\td标签间的内容 网页中常用的布局包括table布局或div布局，其中table表格布局中常见的标签包括tr、th和td，表格行为tr（table row），表格数据为td（table data），表格表头th（table heading）。那么如何抓取这些标签之间的内容呢？下面代码是获取它们之间内容。 假设存在HTML代码如下所示： 表格 学号姓名 1001杨秀璋 1002严娜 则爬取对应值的Python代码如下： # coding=utf-8 import re import urllib content = urllib.urlopen(\"test.html\").read() #打开本地文件 #获取间内容 res = r'(.*?)' texts = re.findall(res, content, re.S|re.M) for m in texts: print m \"\"\" texts: ['学号姓名', '1001杨秀璋', '1002严娜'] \"\"\" #获取间内容 for m in texts: res_th = r'(.*?)' m_th = re.findall(res_th, m, re.S|re.M) for t in m_th: print t #直接获取间内容 res = r'(.*?)(.*?)' texts = re.findall(res, content, re.S|re.M) \"\"\" texts: [('1001', '杨秀璋'), ('1002', '严娜')] \"\"\" for m in texts: print m[0],m[1] 输出结果如下，首先获取tr之间的内容，然后再在tr之间内容中获取和之间值，即“学号”、“姓名”，最后讲述直接获取两个之间的内容方法。 >>> 学号姓名 1001杨秀璋 1002严娜 学号 姓名 1001 杨秀璋 1002 严娜 >>> 抓取标签中的参数 抓取超链接标签的URL HTML超链接的基本格式为链接内容，现在需要获取其中的URL链接地址，方法如下： # coding=utf-8 import re content = ''' 新闻 hao123 地图 视频 ''' res = r\"(? 输出内容如下： >>> http://news.baidu.com http://www.hao123.com http://map.baidu.com http://v.baidu.com >>> (? 解释 如果表达式为“(? (?=) 解释 如果表达式为“(?=.com)”，意思是：一个字符串后面跟着“.com”才做匹配操作，并不使用任何目标字符串 抓取图片超链接标签的URL HTML插入图片使用标签的基本格式为，则需要获取图片URL链接地址的方法如下： content = '''''' urls = re.findall('src=\"(.*?)\"', content, re.I|re.S|re.M) print urls # ['http://www..csdn.net/eastmount.jpg'] 其中图片对应的超链接为http://www..csdn.net/eastmount.jpg，这些资源通常存储在服务器端，最后一个“/”后面的字段即为资源的名称，该图片名称为“eastmount.jpg”。那么如何获取URL中最后一个参数呢？ 获取URL中最后一个参数 通常在使用Python爬取图片过程中，会遇到图片对应的URL最后一个字段通常用于命名图片，如前面的“eastmount.jpg”，需要通过URL“/”后面的参数获取图片。 content = '''''' urls = 'http://www..csdn.net/eastmount.jpg' name = urls.split('/')[-1] print name # eastmount.jpg 该段代码表示采用字符“/”分割字符串，并且获取最后一个获取的值，即为图片名称。 字符串处理及替换 在使用正则表达式爬取网页文本时，通常需要调用find() 函数找到指定的位置，再进行进一步爬取，比如获取class属性为“infobox”的表格table，再进行定位爬取。 start = content.find(r'') #重点点位置 infobox = text[start:end] print infobox 同时爬取过程中可能会爬取到无关变量，此时需要对无关内容进行过滤，这里推荐使用replace函数和正则表达式进行处理。比如，爬取内容如下： # coding=utf-8 import re content = ''' 1001杨秀璋 1002颜 娜 1003Python ''' res = r'(.*?)(.*?)' texts = re.findall(res, content, re.S|re.M) for m in texts: print m[0],m[1] 输出如下所示： >>> 1001 杨秀璋 1002 颜 娜 1003 Python >>> 此时需要过滤多余字符串，如换行（）、空格（ ）、加粗（）。 过滤代码如下： # coding=utf-8 import re content = ''' 1001杨秀璋 1002颜 娜 1003Python ''' res = r'(.*?)(.*?)' texts = re.findall(res, content, re.S|re.M) for m in texts: value0 = m[0].replace('', '').replace(' ', '') value1 = m[1].replace('', '').replace(' ', '') if '' in value1: m_value = re.findall(r'(.*?)', value1, re.S|re.M) print value0, m_value[0] else: print value0, value1 采用replace将字符串“”或“' ”替换成空白，实现过滤，而加粗（）需要使用正则表达式过滤，输出结果如下： >>> 1001 杨秀璋 1002 颜娜 1003 Python >>> 参考 [python爬虫] 正则表达式使用技巧及爬取个人博客实例 Update time： 2020-05-26 14:41 "},"Chapter1/识别文件的编码格式.html":{"url":"Chapter1/识别文件的编码格式.html","title":"识别文件的编码格式","keywords":"","body":"识别文件的编码格式 chardet库文档:https://chardet.readthedocs.io/en/latest/usage.html 模块介绍 Chardet：通用字符编码检测器 检测字符集范围： ASCII，UTF-8，UTF-16（2种变体），UTF-32（4种变体） Big5，GB2312，EUC-TW，HZ-GB-2312，ISO-2022-CN（繁体中文和简体中文） EUC-JP，SHIFT_JIS，CP932，ISO-2022-JP（日文） EUC-KR，ISO-2022-KR（韩文） KOI8-R，MacCyrillic，IBM855，IBM866，ISO-8859-5，windows-1251（西里尔文） ISO-8859-5，windows-1251（保加利亚语） ISO-8859-1，windows-1252（西欧语言） ISO-8859-7，windows-1253（希腊语） ISO-8859-8，windows-1255（视觉和逻辑希伯来语） TIS-620（泰国语）d'y 当python程序中某一个数据文件不知道编码时，可使用chardet第三方库来检测，代码如下（path中填对应文件路径即可) import chardet if __name__ == '__main__': path='***' f=open(path,'rb') data=f.read() print(chardet.detect(data)) # {'language': '', 'confidence': 0.73, 'encoding': 'Windows-1252'} detect函数只需要一个 非unicode字符串参数，返回一个字典。该字典包括判断到的编码格式及判断的置信度。 chardet.detect() 的返回值，为一个字典： {'language': '', 'confidence': 0.73, 'encoding': 'Windows-1252'} 得到文件的编码方式，可以才采用字典的方式 codedetect = chardet.detect(data)[\"encoding\"] #检测得到编码方式 将获取的内容进行解码 code = chardet.detect(response.content)[\"encoding\"] # 获取编码格式 response.encoding = code # 指定编码格式 return response.text Update time： 2020-05-26 14:48 "},"Chapter1/fake-useragent.html":{"url":"Chapter1/fake-useragent.html","title":"fake-useragent","keywords":"","body":"fake-useragent Python库(fake-useragent),可以随机生成各种UserAgent 安装 pip install fake-useragent 基本使用 from fake_useragent import UserAgent ua = UserAgent() from fake_useragent import UserAgent ua = UserAgent() ua.ie # Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US); ua.msie # Mozilla/5.0 (compatible; MSIE 10.0; Macintosh; Intel Mac OS X 10_7_3; Trident/6.0)' ua['Internet Explorer'] # Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.1; Trident/4.0; GTB7.4; InfoPath.2; SV1; .NET CLR 3.3.69573; WOW64; en-US) ua.opera # Opera/9.80 (X11; Linux i686; U; ru) Presto/2.8.131 Version/11.11 ua.chrome # Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.2 (KHTML, like Gecko) Chrome/22.0.1216.0 Safari/537.2' ua.google # Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_4) AppleWebKit/537.13 (KHTML, like Gecko) Chrome/24.0.1290.1 Safari/537.13 ua['google chrome'] # Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11 ua.firefox # Mozilla/5.0 (Windows NT 6.2; Win64; x64; rv:16.0.1) Gecko/20121011 Firefox/16.0.1 ua.ff # Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:15.0) Gecko/20100101 Firefox/15.0.1 ua.safari # Mozilla/5.0 (iPad; CPU OS 6_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/6.0 Mobile/10A5355d Safari/8536.25 # and the best one, random via real world browser usage statistic ua.random 注意: fake-useragent 将收集到的数据缓存到temp文件夹, 例如 /tmp, 更新数据: from fake_useragent import UserAgent ua = UserAgent() ua.update() 有时候会因为网络或者其他问题,出现异常(fake_useragent.errors.FakeUserAgentError: Maximum amount of retries reached), 可以禁用服务器缓存 from fake_useragent import UserAgent ua = UserAgent(use_cache_server=False) 可以自己添加本地数据文件 import fake_useragent # I am STRONGLY!!! recommend to use version suffix location = '/home/user/fake_useragent%s.json' % fake_useragent.VERSION ua = fake_useragent.UserAgent(path=location) ua.random 参考 fake-useragent 0.1.11 Update time： 2020-05-26 16:25 "},"Chapter1/AJAX动态网页数据抓取、Selenium使用.html":{"url":"Chapter1/AJAX动态网页数据抓取、Selenium使用.html","title":"AJAX动态网页数据抓取、Selenium使用","keywords":"","body":"AJAX动态网页数据抓取、Selenium使用 什么是AJAX AJAX（Asynchronouse JavaScript And XML）异步JavaScript和XML。过在后台与服务器进行少量数据交换，Ajax 可以使网页实现异步更新。这意味着可以在不重新加载整个网页的情况下，对网页的某部分进行更新。传统的网页（不使用Ajax）如果需要更新内容，必须重载整个网页页面。因为传统的在传输数据格式方面，使用的是XML语法。因此叫做AJAX，其实现在数据交互基本上都是使用JSON。使用AJAX加载的数据，即使使用了JS，将数据渲染到了浏览器中，在右键->查看网页源代码还是不能看到通过ajax加载的数据，只能看到使用这个url加载的html代码。 获取ajax数据的方式 直接分析ajax调用的接口。然后通过代码请求这个接口。 使用Selenium+chromedriver模拟浏览器行为获取数据。 接口分析 豆瓣电影爬取 https://movie.douban.com/typerank?type_name=%E5%96%9C%E5%89%A7&type=24& interval_id=100:90&action= 打开页面，电影页面没有分页显示按钮，当鼠标向下滚动时，会加载新的电影， 查看网页，查看Request URL:分析 https://movie.douban.com/j/chart/top_list?type=24& interval_id=100%3A90&action=&start=0&limit=20 URL中的 start 参数是动态变化的 测试，能获取‘一页’的电影 import requests url = \"https://movie.douban.com/j/chart/top_list?type=24& interval_id=100%3A90&action=&start=20&limit=20\" res = requests.get(url) print(res.text) 测试，获取所有电影 import requests base_url = \"https://movie.douban.com/j/chart/top_list?type=24& interval_id=100%3A90&action=&start={}&limit=20\" #可以适当的增加 limit= 的值 提高效率 i = 0 while True: print(i) url=base_url.format(i*20) res=requests.get(url) info = res.text print(len(info)) if len(info) == 0: break i += 1 Selenium+chromedriver获取动态数据 Selenium 相当于是一个机器人。可以模拟人类在浏览器上的一些行为，自动处理浏览器上的一些行为，比如点击，填充数据，删除cookie等。chromedriver是一个驱动Chrome浏览器的驱动程序，使用他才可以驱动浏览器。当然针对不同的浏览器有不同的driver。以下列出了不同浏览器及其对应的driver： Chrome Firefox Edge Safari 安装Selenium和chromedriver 安装Selenium：Selenium有很多语言的版本，有java、ruby、python等。我们下载python版本的就可以了 pip install selenium 安装chromedriver：下载完成后，放到不需要权限的纯英文目录下就可以了。 快速入门 现在以一个简单的获取百度首页的例子来讲下Selenium和chromedriver如何快速入门： from selenium import webdriver # chromedriver的绝对路径 driver_path = r'D:\\ProgramApp\\chromedriver\\chromedriver.exe' # 初始化一个driver，并且指定chromedriver的路径 driver = webdriver.Chrome(executable_path=driver_path) # 请求网页 driver.get(\"https://www.baidu.com/\") # 通过page_source获取网页代码（浏览器渲染完后的代码） print(driver.page_source) selenium常用操作 更多教程请参考 定位元素方法 Selenium提供了一下方法来定义一个页面中的元素： find_element_by_id find_element_by_name find_element_by_xpath find_element_by_link_text find_element_by_partial_link_text find_element_by_tag_name find_element_by_class_name find_element_by_css_selector 下面是查找多个元素（这些方法将返回一个列表） find_elements_by_name find_elements_by_xpath find_elements_by_link_text find_elements_by_partial_link_text find_elements_by_tag_name find_elements_by_class_name find_elements_by_css_selector 常用方法是通过xpath相对路径进行定位, 例如 定位username元素的方法如下： username = driver.find_element_by_xpath(\"//form[input/@name='username']\") username = driver.find_element_by_xpath(\"//form[@id='loginForm']/input[1]\") username = driver.find_element_by_xpath(\"//input[@name='username']\") [1] 第一个form元素通过一个input子元素，name属性和值为username实现 [2] 通过id=loginForm值的form元素找到第一个input子元素 [3] 属性名为name且值为username的第一个input元素 操作元素方法 通常所有的操作与页面交互都将通过WebElement接口，常见的操作元素方法如下： clear 清除元素的内容 send_keys 模拟按键输入 click 点击元素 submit 提交表单 举例自动访问FireFox浏览器自动登录163邮箱。 from selenium import webdriver from selenium.webdriver.common.keys import Keys import time # Login 163 email driver = webdriver.Firefox() driver.get(\"http://mail.163.com/\") elem_user = driver.find_element_by_name(\"username\") elem_user.clear elem_user.send_keys(\"15201615157\") elem_pwd = driver.find_element_by_name(\"password\") elem_pwd.clear elem_pwd.send_keys(\"******\") elem_pwd.send_keys(Keys.RETURN) #driver.find_element_by_id(\"loginBtn\").click() #driver.find_element_by_id(\"loginBtn\").submit() time.sleep(5) assert \"baidu\" in driver.title driver.close() driver.quit() 首先通过name定位用户名和密码，再调用方法clear()清除输入框默认内容，如“请输入密码”等提示，通过send_keys(\"**\")输入正确的用户名和密码，最后通过click()点击登录按钮或send_keys(Keys.RETURN)相当于回车登录，submit()提交表单。 PS：如果需要输入中文，防止编码错误使用send_keys(u\"中文用户名\")。 WebElement接口获取值 通过WebElement接口可以获取常用的值，这些值同样非常重要。 size 获取元素的尺寸 text 获取元素的文本 get_attribute(name) 获取属性值 location 获取元素坐标，先找到要获取的元素，再调用该方法 page_source 返回页面源码 driver.title 返回页面标题 current_url 获取当前页面的URL is_displayed() 设置该元素是否可见 is_enabled() 判断元素是否被使用 is_selected() 判断元素是否被选中 tag_name 返回元素的tagName from selenium import webdriver from selenium.webdriver.common.keys import Keys import time driver = webdriver.PhantomJS(executable_path=\"G:\\phantomjs-1.9.1-windows\\phantomjs.exe\") driver.get(\"http://www.baidu.com/\") size = driver.find_element_by_name(\"wd\").size print size #尺寸: {'width': 500, 'height': 22} news = driver.find_element_by_xpath(\"//div[@id='u1']/a[1]\").text print news #文本: 新闻 href = driver.find_element_by_xpath(\"//div[@id='u1']/a[2]\").get_attribute('href') name = driver.find_element_by_xpath(\"//div[@id='u1']/a[2]\").get_attribute('name') print href,name #属性值: http://www.hao123.com/ tj_trhao123 location = driver.find_element_by_xpath(\"//div[@id='u1']/a[3]\").location print location #坐标: {'y': 19, 'x': 498} print driver.current_url #当前链接: https://www.baidu.com/ print driver.title #标题: 百度一下， 你就知道 result = location = driver.find_element_by_id(\"su\").is_displayed() print result #是否可见: True 鼠标操作 在现实的自动化测试中关于鼠标的操作不仅仅是click()单击操作，还有很多包含在ActionChains类中的操作。如下： context_click(elem) 右击鼠标点击元素elem，另存为等行为 double_click(elem) 双击鼠标点击元素elem，地图web可实现放大功能 drag_and_drop(source,target) 拖动鼠标，源元素按下左键移动至目标元素释放 move_to_element(elem) 鼠标移动到一个元素上 click_and_hold(elem) 按下鼠标左键在一个元素上 perform() 在通过调用该函数执行ActionChains中存储行为 关闭浏览器 driver.close()：关闭当前页面。 driver.quit()：退出整个浏览器。 方法 区别 close 关闭当前的浏览器窗口 quit 不仅关闭窗口，还会彻底的退出webdriver，释放与driver server之间的连接 页面等待 现在的网页越来越多采用了 Ajax 技术，这样程序便不能确定何时某个元素完全加载出来了。如果实际页面等待时间过长导致某个dom元素还没出来，但是你的代码直接使用了这个WebElement，那么就会抛出NullPointer的异常。为了解决这个问题。所以 Selenium 提供了两种等待方式：一种是隐式等待、一种是显式等待。 隐式等待：调用driver.implicitly_wait。那么在获取不可用的元素之前，会先等待10秒中的时间。示例代码如下： driver = webdriver.Chrome(executable_path=driver_path) driver.implicitly_wait(10) # 请求网页 driver.get(\"https://www.douban.com/\") selenium_通过selenium控制浏览器滚动条 目的：通过selenium控制浏览器滚动条 原理：通过 driver.execute_script()执行js代码，达到目的 driver.execute_script(\"window.scrollBy(0,1000)\") 语法：scrollBy(x,y) 参数 描述 x 必需。向右滚动的像素值。 y 必需。向下滚动的像素值。 或者使用 js=\"var q=document.documentElement.scrollTop=10000\" driver.execute_script(js) 例如在京东的某个页面，当鼠标向下滚动的时候才会把整个页面的商品加载出来，利用 driver.execute_script() 可以设置某个阈值将页面一次滑倒最底端 from selenium import webdriver from lxml import etree from time import sleep url = 'https://search.jd.com/Search?keyword=mac&enc=utf-8&wq=mac&pvid=9862d03c24e741c6a58079d004f5aabf' chrome = webdriver.Chrome() chrome.get(url) js = 'document.documentElement.scrollTop=100000' chrome.execute_script(js) sleep(3) html = chrome.page_source e = etree.HTML(html) prices = e.xpath('//div[@class=\"gl-i-wrap\"]/div[@class=\"p-price\"]/strong/i/text()') names = e.xpath('//div[@class=\"gl-i-wrap\"]/div[@class=\"p-name p-name-type-2\"]/a/em') print(len(names)) for name, price in zip(names, prices): print(name.xpath('string(.)'), \":\", price) chrome.quit() selenium操作无界面chrome浏览器 from selenium import webdriver from selenium.webdriver.chrome.options import Options req_url = \"https://www.baidu.com\" chrome_options=Options() #设置chrome浏览器无界面模式 chrome_options.add_argument('--headless') browser = webdriver.Chrome(chrome_options=chrome_options) # 开始请求 browser.get(req_url) #打印页面源代码 print(browser.page_source) #关闭浏览器 browser.close() #关闭chreomedriver进程 browser.quit() selenium爬取局部动态刷新网站（URL始终固定） 测试，虎牙直播，当进入某个直播分类的时候，点击不同的分页，URL不发生变化，可以通过selenium的click()事件，实现翻页的情况， from selenium import webdriver from lxml import etree import time import requests driver_path=r'D:\\chromedriver_win32\\chromedriver.exe' driver = webdriver.Chrome(executable_path=driver_path) content=[] url = 'https://www.huya.com/g/wzry' driver.get(url) driver.implicitly_wait(1) m=1 def page(): global m global content while m 参考 [python爬虫] Selenium常见元素定位方法和操作的学习介绍 Selenium-Python中文文档 Update time： 2020-05-26 16:14 "}}