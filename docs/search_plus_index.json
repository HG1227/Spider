{"./":{"url":"./","title":"Introduction","keywords":"","body":"Introduction Update time： 2020-05-26 "},"Chapter1/":{"url":"Chapter1/","title":"基础","keywords":"","body":"基础 Update time： 2020-05-26 "},"Chapter1/requests库.html":{"url":"Chapter1/requests库.html","title":"requests库","keywords":"","body":"requests库 安装 使用pip进行安装 要安装 requests，最方便快捷的方法是使用 pip 进行安装。 pip install requests 如果还没有安装pip，这个链接 Properly Installing Python 详细介绍了在各种平台下如何安装 python 以及 setuptools，pip，virtualenv 等常用的 python 工具，可以参考其中的步骤来进行安装。 get或post请求 # 不带可选参数的get请求 >>> r = requests.get(url='https://github.com/timeline.json') # 不带可选参数的post请求 >>> r = requests.post(url=\"http://httpbin.org/post\") get\\post参数说明 http 请求 get 与 post 是最常用的，url 为必选参数，常用常用参数有params、data、json、files、timeout、headers、cookies；其他基本用不到的有verify，cert，auth，allow_redirects，proxies，hooks，stream。 下面列表对具体的参数说明： 重点在 params、data、json、files、timeout，其次headers、cookies，其他略过就可以。 get请求文本与二进制数据内容(图片) get请求是最简单的、发送的数据量比较小。 示例2.1: 带多个参数的请求，返回文本数据 import requests # 带参数的GET请求,timeout请求超时时间 params = {'key1': 'python', 'key2': 'java'} r = requests.get(url='http://httpbin.org/get', params=params, timeout=3) # 注意观察url地址，它已经将参数拼接起来 print('URL地址：', r.url) # 响应状态码，成功返回200，失败40x或50x print('请求状态码：', r.status_code) print('header信息:', r.headers) print('cookie信息：', r.cookies) print('响应的数据：', r.text) # 如响应是json数据 ，可以使用 r.json()自动转换为dict print('响应json数据', r.json()) 示例2.2：get 返回二进制数据，如图片。 from PIL import Image from io import BytesIO import requests # 请求获取图片并保存 r = requests.get('https://pic3.zhimg.com/247d9814fec770e2c85cc858525208b2_is.jpg') i = Image.open(BytesIO(r.content)) # i.show() # 查看图片 # 将图片保存 with open('img.jpg', 'wb') as fd: for chunk in r.iter_content(): fd.write(chunk) post请求，上传表单，文本，文件\\图片 post 请求比 get 复杂，请求的数据有多种多样，有表单(form-data)，文本(json\\xml等)，文件流（图片\\文件）等。 表单形式提交的post请求，只需要将数据传递给post()方法的data参数。见示例3.1. json文本形式提交的post请求，一种方式是将json数据dumps后传递给data参数，另一种方式就是直接将json数据传递给post()方法的json参数。见示例3.2. 单个文件提交的post请求，将文件流给post()方法的files参数。见示例3.3。 多个文件提交的post请求，将文件设到一个元组的列表中，其中元组结构为 (form_field_name, file_info)；然后将数据传递给post()方法的files。见示例3.4 示例3.1：post 表单请求 import requests, json # 带参数表单类型post请求 data={'custname': 'woodman','custtel':'13012345678','custemail':'woodman@11.com', 'size':'small'} r = requests.post('http://httpbin.org/post', data=data) print('响应数据：', r.text) 示例3.2：post json请求 # json数据请求 url = 'https://api.github.com/some/endpoint' payload = {'some': 'data'} # 可以使用json.dumps(dict) 对编码进行编译 r = requests.post(url, data=json.dumps(payload)) print('响应数据：', r.text) # 可以直接使用json参数传递json数据 r = requests.post(url, json=payload) print('响应数据：', r.text) 示例3.3：post提交单个文件 # 上传单个文件 url = 'http://httpbin.org/post' # 注意文件打开的模式，使用二进制模式不容易发生错误 files = {'file': open('report.txt', 'rb')} # 也可以显式地设置文件名，文件类型和请求头 # files = {'file': ('report.xls', open('report.xls', 'rb'), 'application/vnd.ms-excel', {'Expires': '0'})} r = requests.post(url, files=files) r.encoding = 'utf-8' print(r.text) 注意：文件的上传使用二进制打开不容易报错。 示例3.4：上传多个文件 url = 'http://httpbin.org/post' multiple_files = [ ('images', ('foo.png', open('foo.png', 'rb'), 'image/png')), ('images', ('bar.png', open('bar.png', 'rb'), 'image/png'))] r = requests.post(url, files=multiple_files) print(r.text) get与post请求的header与cookie管理 获取 get 与 post 请求响应的 header 与 cookie 分别使用 r.headers 与r.cookies 。如果提交请求数据是对 header 与 cookie 有修改，需要在get()与post()方法中加入 headers 或 cookies 参数，它们值的类型都是字典 示例4.1：定制请求头header import requests url = 'https://api.github.com/some/endpoint' headers = {'user-agent': 'my-app/0.0.1'} r = requests.get(url, headers=headers) print(r.headers) # 获取响应数据的header信息 注意：requests自带headers管理，一般情况下不需要设置header信息。Requests 不会基于定制 header 的具体情况改变自己的行为。只不过在最后的请求中，所有的 header 信息都会被传递进去。 示例4.2：定制cookie信息 # 直接以字典型时传递cookie url = 'http://httpbin.org/cookies' cookies = {\"cookies_are\":'working'} r = requests.get(url, cookies=cookies) # 获取响应的cookie信息，返回结果是RequestsCookieJar对象 print(r.cookies) print(r.text) session与cookie存储 如果你向同一主机发送多个请求，每个请求对象让你能够跨请求保持session和cookie信息，这时我们要使用到requests的Session()来保持回话请求的cookie和session与服务器的相一致。 import requests url = \"http://www.renren.com/PLogin.do\" data = {\"email\":\"970138074@qq.com\",'password':\"pythonspider\"} headers = { 'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36\" } # 登录 session = requests.session() session.post(url,data=data,headers=headers) # 访问大鹏个人中心 resp = session.get('http://www.renren.com/880151247/profile') print(resp.text) requests请求返回对象Response的常用方法 requests.get(url) 与 requests.post(url) 的返回对象为Response 类对象。 Response响应类常用属性与方法： Response.url 请求url，[见示例2.1] Response.status_code 响应状态码，[见示例2.1] Response.text 获取响应内容，[见示例2.1] Response.content 以字节形式获取响应提，多用于非文本请求，[见示例2.2] Response.json() 活动响应的JSON内容，[见示例2.1] Response.ok 请求是否成功，status_code 参考 python3之requests Update time： 2020-05-26 "},"Chapter1/XPath语法和lxml模块.html":{"url":"Chapter1/XPath语法和lxml模块.html","title":"XPath语法和lxml模块","keywords":"","body":"XPath语法和lxml模块 什么是XPath？ xpath（XML Path Language）是一门在XML和HTML文档中查找信息的语言，可用来在XML和HTML文档中对元素和属性进行遍历。 XPath语法 选取节点： XPath 使用路径表达式来选取 XML 文档中的节点或者节点集。这些路径表达式和我们在常规的电脑文件系统中看到的表达式非常相似。 表达式 描述 示例 结果 nodename 选取此节点的所有子节点 bookstore 选取bookstore下所有的子节点 / 如果是在最前面，代表从根节点选取。否则选择某节点下的某个节点 /bookstore 选取根元素下所有的bookstore节点 // 从全局节点中选择节点，随便在哪个位置 //book 从全局节点中找到所有的book节点 @ 选取某个节点的属性 //book[@price] 选择所有拥有price属性的book节点 . 当前节点 ./a 选取当前节点下的a标签 谓语： 谓语用来查找某个特定的节点或者包含某个指定的值的节点，被嵌在方括号中。 在下面的表格中，我们列出了带有谓语的一些路径表达式，以及表达式的结果： 通配符 *表示通配符 通配符 描述 示例 结果 * 匹配任意节点 /bookstore/* 选取bookstore下的所有子元素。 @* 匹配节点中的任何属性 //book[@*] 选取所有带有属性的book元素。 选取多个路径： 通过在路径表达式中使用 “|” 运算符，可以选取若干个路径。 //bookstore/book | //book/title # 选取所有book元素以及book元素下所有的title元素 lxml库 lxml 是 一个HTML/XML的解析器，主要的功能是如何解析和提取 HTML/XML 数据。 lxml和正则一样，也是用 C 实现的，是一款高性能的 Python HTML/XML 解析器，我们可以利用之前学习的XPath语法，来快速的定位特定元素以及节点信息。 基本使用： 解析HTML代码，并且在解析HTML代码的时候，如果HTML代码不规范，他会自动的进行补全。示例代码如下： # 使用 lxml 的 etree 库 from lxml import etree text = ''' first item second item third item fourth item fifth item # 注意，此处缺少一个 闭合标签 ''' #利用etree.HTML，将字符串解析为HTML文档 html = etree.HTML(text) # 按字符串序列化HTML文档 result = etree.tostring(html) print(result) 输入结果如下： first item second item third item fourth item fifth item 可以看到。lxml会自动修改HTML代码。例子中不仅补全了li标签，还添加了body，html标签。 从文件中读取html代码 除了直接使用字符串进行解析，lxml还支持从文件中读取内容。我们新建一个hello.html文件： first item second item third item fourth item fifth item 然后利用 etree.parse()方法来读取文件。示例代码如下： from lxml import etree # 读取外部文件 hello.html html = etree.parse('hello.html') result = etree.tostring(html, pretty_print=True) print(result) 在lxml中使用XPath语法： 读取文本解析节点 from lxml import etree text=''' 第一个 second item a属性 ''' html=etree.HTML(text) #初始化生成一个XPath解析对象 result=etree.tostring(html,encoding='utf-8') #解析对象输出代码 print(type(html)) print(type(result)) print(result.decode('utf-8')) #etree会修复HTML文本节点 第一个 second item a属性 读取HTML文件进行解析 from lxml import etree html=etree.parse('test.html',etree.HTMLParser()) #指定解析器HTMLParser会根据文件修复HTML文件中缺失的如声明信息 result=etree.tostring(html) #解析成字节 #result=etree.tostringlist(html) #解析成列表 print(type(html)) print(type(result)) print(result) # b'\\n \\n \\n first item \\n second item \\n third item \\n fourth item \\n fifth item \\n \\n \\n' 获取所有节点 返回一个列表每个元素都是Element类型，所有节点都包含在其中 from lxml import etree html=etree.parse('test',etree.HTMLParser()) result=html.xpath('//*') #//代表获取子孙节点，*代表获取所有 print(type(html)) print(type(result)) print(result) # [, , , , , , , , , , , , , ] 如要获取li节点，可以使用//后面加上节点名称，然后调用xpath()方法 html.xpath('//li') #获取所有子孙节点的li节点 获取子节点 通过/或者//即可查找元素的子节点或者子孙节点，如果想选择li节点的所有直接a节点，可以这样使用 result=html.xpath('//li/a') #通过追加/a选择所有li节点的所有直接a节点， # 因为//li用于选中所有li节点，/a用于选中li节点的所有直接子节点a 属性匹配 在选取的时候，我们还可以用@符号进行属性过滤。比如，这里如果要选取class为item-1的li节点，可以这样实现: from lxml import etree from lxml.etree import HTMLParser text=''' 第一个 second item ''' html=etree.HTML(text,etree.HTMLParser()) result=html.xpath('//li[@class=\"item-1\"]') print(result) 文本获取 我们用XPath中的text()方法获取节点中的文本 from lxml import etree text=''' 第一个 second item ''' html=etree.HTML(text,etree.HTMLParser()) result=html.xpath('//li[@class=\"item-1\"]/a/text()') #获取a节点下的内容 result1=html.xpath('//li[@class=\"item-1\"]//text()') #获取li下所有子孙节点的内容 print(result) print(result1) 属性获取 使用@符号即可获取节点的属性，如下：获取所有li节点下所有a节点的href属性 result=html.xpath('//li/a/@href') #获取a的href属性 result=html.xpath('//li//@href') #获取所有li子孙节点的href属性 属性多值匹配 如果某个属性的值有多个时，我们可以使用contains()函数来获取 from lxml import etree text1=''' 第一个 second item ''' html=etree.HTML(text1,etree.HTMLParser()) result=html.xpath('//li[@class=\"aaa\"]/a/text()') result1=html.xpath('//li[contains(@class,\"aaa\")]/a/text()') print(result) print(result1) #通过第一种方法没有取到值，通过contains（）就能精确匹配到节点了 [] ['第一个'] 多属性匹配 另外我们还可能遇到一种情况，那就是根据多个属性确定一个节点，这时就需要同时匹配多个属性，此时可用运用and运算符来连接使用： from lxml import etree text1=''' 第一个 second item ''' html=etree.HTML(text1,etree.HTMLParser()) result=html.xpath('//li[@class=\"aaa\" and @name=\"fore\"]/a/text()') result1=html.xpath('//li[contains(@class,\"aaa\") and @name=\"fore\"]/a/text()') print(result) print(result1) # ['second item'] ['second item'] 按序选择 有时候，我们在选择的时候某些属性可能同时匹配多个节点，但我们只想要其中的某个节点，如第二个节点或者最后一个节点，这时可以利用中括号引入索引的方法获取特定次序的节点： from lxml import etree text1=''' 第一个 第二个 第三个 第四个 ''' html=etree.HTML(text1,etree.HTMLParser()) result=html.xpath('//li[contains(@class,\"aaa\")]/a/text()') #获取所有li节点下a节点的内容 result1=html.xpath('//li[1][contains(@class,\"aaa\")]/a/text()') #获取第一个 result2=html.xpath('//li[last()][contains(@class,\"aaa\")]/a/text()') #获取最后一个 result3=html.xpath('//li[position()>2 and position() 这里使用了last()、position()函数，在XPath中，提供了100多个函数，包括存取、数值、字符串、逻辑、节点、序列等处理功能，它们的具体作用可参考：http://www.w3school.com.cn/xpath/xpath_functions.asp lxml结合xpath注意事项： 使用xpath语法。应该使用Element.xpath方法。来执行xpath的选择。示例代码如下 trs = html.xpath(\"//tr[position()>1]\") xpath函数返回来的永远是一个列表。 获取某个标签的属性： href = html.xpath(\"//a/@href\") # 获取a标签的href属性对应的值 若再某个标签下 也可以直接用get()函数 html = etree.HTML(text) imgs = html.xpath(\"//div[@class='page-content text-center']//img[@class!='gif']\") for img in imgs: img_url = img.get('data-original') alt = img.get('alt') 获取文本，是通过xpath中的text()函数。示例代码如下： address = tr.xpath(\"./td[4]/text()\")[0] 在某个标签下，再执行xpath函数，获取这个标签下的子孙元素，那么应该在斜杠之前加一个点. ，代表是在当前元素下获取。示例代码如下： address = tr.xpath(\"./td[4]/text()\")[0] # address = tr.xpath(\"./td[4]//text()\")[0] 子孙标签的所有文本 需要注意的知识点： / 和// 的区别：/ 代表只获取直接子节点。// 获取子孙节点。一般// 用得比较多。当然也要视情况而定。 contains：有时候某个属性中包含了多个值，那么可以使用contains函数。示例代码如下：//div[contains(@class,'job_detail')] 谓词中的下标是从1开始的，不是从0开始的。 练习 #encoding: utf-8 from lxml import etree # 1. 获取所有tr标签 # 2. 获取第2个tr标签 # 3. 获取所有class等于even的tr标签 # 4. 获取所有a标签的href属性 # 5. 获取所有的职位信息（纯文本） parser = etree.HTMLParser(encoding='utf-8') html = etree.parse(\"tencent.html\",parser=parser) # 1. 获取所有tr标签 # //tr # xpath函数返回的是一个列表 # trs = html.xpath(\"//tr\") # for tr in trs: # print(etree.tostring(tr,encoding='utf-8').decode(\"utf-8\")) # 2. 获取第2个tr标签 # tr = html.xpath(\"//tr[2]\")[0] # print(etree.tostring(tr,encoding='utf-8').decode(\"utf-8\")) # 3. 获取所有class等于even的tr标签 # trs = html.xpath(\"//tr[@class='even']\") # for tr in trs: # print(etree.tostring(tr,encoding='utf-8').decode(\"utf-8\")) # 4. 获取所有a标签的href属性 # aList = html.xpath(\"//a/@href\") # for a in aList: # print(\"http://hr.tencent.com/\"+a) # 5. 获取所有的职位信息（纯文本） trs = html.xpath(\"//tr[position()>1]\") positions = [] for tr in trs: # 在某个标签下，再执行xpath函数，获取这个标签下的子孙元素 # 那么应该在//之前加一个点，代表是在当前元素下获取 href = tr.xpath(\".//a/@href\")[0] fullurl = 'http://hr.tencent.com/' + href title = tr.xpath(\"./td[1]//text()\")[0] category = tr.xpath(\"./td[2]/text()\")[0] nums = tr.xpath(\"./td[3]/text()\")[0] address = tr.xpath(\"./td[4]/text()\")[0] pubtime = tr.xpath(\"./td[5]/text()\")[0] position = { 'url': fullurl, 'title': title, 'category': category, 'nums': nums, 'address': address, 'pubtime': pubtime } positions.append(position) print(positions) 案例应用：抓取TIOBE指数前20名排行开发语言 #!/usr/bin/env python #coding:utf-8 import requests from requests.exceptions import RequestException from lxml import etree from lxml.etree import ParseError import json def one_to_page(html): headers={ 'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.62 Safari/537.36' } try: response=requests.get(html,headers=headers) body=response.text #获取网页内容 except RequestException as e: print('request is error!',e) try: html=etree.HTML(body,etree.HTMLParser()) #解析HTML文本内容 result=html.xpath('//table[contains(@class,\"table-top20\")]/tbody/tr//text()') #获取列表数据 pos = 0 for i in range(20): if i == 0: yield result[i:5] else: yield result[pos:pos+5] #返回排名生成器数据 pos+=5 except ParseError as e: print(e.position) def write_file(data): #将数据重新组合成字典写入文件并输出 for i in data: sul={ '2018年6月排行':i[0], '2017年6排行':i[1], '开发语言':i[2], '评级':i[3], '变化率':i[4] } with open('test.txt','a',encoding='utf-8') as f: f.write(json.dumps(sul,ensure_ascii=False) + '\\n') #必须格式化数据 f.close() print(sul) return None def main(): url='https://www.tiobe.com/tiobe-index/' data=one_to_page(url) revaule=write_file(data) if revaule == None: print('ok') if __name__ == '__main__': main() # {'2018年6月排行': '1', '2017年6排行': '1', '开发语言': 'Java', '评级': '15.368%', '变化率': '+0.88%'} {'2018年6月排行': '2', '2017年6排行': '2', '开发语言': 'C', '评级': '14.936%', '变化率': '+8.09%'} {'2018年6月排行': '3', '2017年6排行': '3', '开发语言': 'C++', '评级': '8.337%', '变化率': '+2.61%'} {'2018年6月排行': '4', '2017年6排行': '4', '开发语言': 'Python', '评级': '5.761%', '变化率': '+1.43%'} {'2018年6月排行': '5', '2017年6排行': '5', '开发语言': 'C#', '评级': '4.314%', '变化率': '+0.78%'} {'2018年6月排行': '6', '2017年6排行': '6', '开发语言': 'Visual Basic .NET', '评级': '3.762%', '变化率': '+0.65%'} {'2018年6月排行': '7', '2017年6排行': '8', '开发语言': 'PHP', '评级': '2.881%', '变化率': '+0.11%'} {'2018年6月排行': '8', '2017年6排行': '7', '开发语言': 'JavaScript', '评级': '2.495%', '变化率': '-0.53%'} {'2018年6月排行': '9', '2017年6排行': '-', '开发语言': 'SQL', '评级': '2.339%', '变化率': '+2.34%'} {'2018年6月排行': '10', '2017年6排行': '14', '开发语言': 'R', '评级': '1.452%', '变化率': '-0.70%'} {'2018年6月排行': '11', '2017年6排行': '11', '开发语言': 'Ruby', '评级': '1.253%', '变化率': '-0.97%'} {'2018年6月排行': '12', '2017年6排行': '18', '开发语言': 'Objective-C', '评级': '1.181%', '变化率': '-0.78%'} {'2018年6月排行': '13', '2017年6排行': '16', '开发语言': 'Visual Basic', '评级': '1.154%', '变化率': '-0.86%'} {'2018年6月排行': '14', '2017年6排行': '9', '开发语言': 'Perl', '评级': '1.147%', '变化率': '-1.16%'} {'2018年6月排行': '15', '2017年6排行': '12', '开发语言': 'Swift', '评级': '1.145%', '变化率': '-1.06%'} {'2018年6月排行': '16', '2017年6排行': '10', '开发语言': 'Assembly language', '评级': '0.915%', '变化率': '-1.34%'} {'2018年6月排行': '17', '2017年6排行': '17', '开发语言': 'MATLAB', '评级': '0.894%', '变化率': '-1.10%'} {'2018年6月排行': '18', '2017年6排行': '15', '开发语言': 'Go', '评级': '0.879%', '变化率': '-1.17%'} {'2018年6月排行': '19', '2017年6排行': '13', '开发语言': 'Delphi/Object Pascal', '评级': '0.875%', '变化率': '-1.28%'} {'2018年6月排行': '20', '2017年6排行': '20', '开发语言': 'PL/SQL', '评级': '0.848%', '变化率': '-0.72%'} 使用requests和xpath爬取电影天堂 import requests from lxml import etree BASE_DOMAIN = 'http://www.dytt8.net' HEADERS = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36', 'Referer': 'http://www.dytt8.net/html/gndy/dyzz/list_23_2.html' } def spider(): url = 'http://www.dytt8.net/html/gndy/dyzz/list_23_1.html' resp = requests.get(url,headers=HEADERS) # resp.content：经过编码后的字符串 # resp.text：没有经过编码，也就是unicode字符串 # text：相当于是网页中的源代码了 text = resp.content.decode('gbk') # tree：经过lxml解析后的一个对象，以后使用这个对象的xpath方法，就可以 # 提取一些想要的数据了 tree = etree.HTML(text) # xpath/beautifulsou4 all_a = tree.xpath(\"//div[@class='co_content8']//a\") for a in all_a: title = a.xpath(\"text()\")[0] href = a.xpath(\"@href\")[0] if href.startswith('/'): detail_url = BASE_DOMAIN + href crawl_detail(detail_url) break def crawl_detail(url): resp = requests.get(url,headers=HEADERS) text = resp.content.decode('gbk') tree = etree.HTML(text) create_time = tree.xpath(\"//div[@class='co_content8']/ul/text()\")[0].strip() imgs = tree.xpath(\"//div[@id='Zoom']//img/@src\") # 电影海报 cover = imgs[0] # 电影截图 screenshoot = imgs[1] # 获取span标签下所有的文本 infos = tree.xpath(\"//div[@id='Zoom']//text()\") for index,info in enumerate(infos): if info.startswith(\"◎年　　代\"): year = info.replace(\"◎年　　代\",\"\").strip() if info.startswith(\"◎豆瓣评分\"): douban_rating = info.replace(\"◎豆瓣评分\",'').strip() print(douban_rating) if info.startswith(\"◎主　　演\"): # 从当前位置，一直往下面遍历 actors = [info] for x in range(index+1,len(infos)): actor = infos[x] if actor.startswith(\"◎\"): break actors.append(actor.strip()) print(\",\".join(actors)) if __name__ == '__main__': spider() 参考 XPath语法详解 XPath-语法大全 XPath的更多用法参考：http://www.w3school.com.cn/xpath/index.asp python lxml库的更多用法参考：http://lxml.de/ python3解析库lxml Update time： 2020-05-26 "},"Chapter1/正则表达式匹配.html":{"url":"Chapter1/正则表达式匹配.html","title":"正则表达式匹配","keywords":"","body":"正则表达式匹配 正则表达式 正则表达式（Regular Expression，简称Regex或RE）又称为正规表示法或常规表示法，常常用来检索、替换那些符合某个模式的文本，它首先设定好了一些特殊的字及字符组合，通过组合的“规则字符串”来对表达式进行过滤，从而获取或匹配我们想要的特定内容。它具有灵活、逻辑性和功能性非常的强，能迅速地通过表达式从字符串中找到所需信息的优点。 re模块 Python通过 re 模块提供对正则表达式的支持，使用正则表达式之前需要导入该库。 import re 其基本步骤是先将正则表达式的字符串形式编译为Pattern实例，然后使用Pattern实例处理文本并获得一个匹配（Match）实例，再使用Match实例获得所需信息。常用的函数是 findall，原型如下： findall(string[, pos[, endpos]]) | re.findall(pattern, string[, flags]) 该函数表示搜索字符串string，以列表形式返回全部能匹配的子串。 其中参数re包括三个常见值： re.I(re.IGNORECASE)：忽略大小写（括号内是完整写法） re.M(re.MULTILINE)：允许多行模式 re.S(re.DOTALL)：支持点任意匹配模式 Pattern对象是一个编译好的正则表达式，通过Pattern提供的一系列方法可以对文本进行匹配查找。Pattern不能直接实例化，必须使用re.compile()进行构造。 complie方法 re正则表达式模块包括一些常用的操作函数，比如complie()函数。其原型如下 compile(pattern[,flags] ) 该函数根据包含正则表达式的字符串创建模式对象，返回一个pattern对象。参数flags是匹配模式，可以使用按位或“|”表示同时生效，也可以在正则表达式字符串中指定。Pattern对象是不能直接实例化的，只能通过compile方法得到。 简单举个实例，使用正则表达式获取字符串中的数字内容，如下所示： >>> import re >>> string=\"A1.45，b5，6.45，8.82\" >>> regex = re.compile(r\"\\d+\\.?\\d*\") >>> print regex.findall(string) ['1.45', '5', '6.45', '8.82'] >>> match方法 match方法是从字符串的 pos 下标处起开始匹配 pattern，如果pattern结束时已经匹配，则返回一个Match对象；如果匹配过程中pattern无法匹配，或者匹配未结束就已到达endpos，则返回None。该方法原型如下： match(string[, pos[, endpos]]) | re.match(pattern, string[, flags]) 参数string表示字符串；pos表示下标，pos和endpos的默认值分别为0和len(string)；参数flags用于编译pattern时指定匹配模式。 search方法 search方法用于查找字符串中可以匹配成功的子串。从字符串的pos下标处起尝试匹配pattern，如果pattern结束时仍可匹配，则返回一个Match对象；若无法匹配，则将pos加1后重新尝试匹配；直到pos=endpos时仍无法匹配则返回None。 函数原型如下： search(string[, pos[, endpos]]) | re.search(pattern, string[, flags]) 参数string表示字符串；pos表示下标，pos和endpos的默认值分别为0和len(string))；参数flags用于编译pattern时指定匹配模式。 group和groups方法 group([group1, …])方法用于获得一个或多个分组截获的字符串，当它指定多个参数时将以元组形式返回。groups([default])方法以元组形式返回全部分组截获的字符串，相当于调用group(1,2,…last)。default表示没有截获字符串的组以这个值替代，默认为None。 正则表达式抓取网络数据常见方法 HTML语言是采用标签对的形式来编写网站的，包括起始标签和结束标签，比如、、等。下面讲解抓取标签对之间的文本内容。 抓取title标签间的内容 首先爬取网页的标题，采用的正则表达式为(.*?)，爬取百度标题代码如下： # coding=utf-8 import re import urllib url = \"http://www.baidu.com/\" content = urllib.urlopen(url).read() title = re.findall(r'(.*?)', content) print title[0] # 百度一下，你就知道 抓取超链接标签间的内容 在HTML中，用于标识超链接，获取完整的超链接和超链接和之间的内容 # coding=utf-8 import re import urllib url = \"http://www.baidu.com/\" content = urllib.urlopen(url).read() #获取完整超链接 res = r\"\" urls = re.findall(res, content) for u in urls: print unicode(u,'utf-8') #获取超链接和之间内容 res = r'(.*?)' texts = re.findall(res, content, re.S|re.M) for t in texts: print unicode(t,'utf-8') 输出结果部分内容如下所示，这里如果直接输出print u或print t可能会乱码，需要调用函数unicode(u,'utf-8')进行转码 #获取完整超链接 新闻 hao123 地图 视频 ... #获取超链接和之间内容 新闻 hao123 地图 视频 ... 抓取tr\\td标签间的内容 网页中常用的布局包括table布局或div布局，其中table表格布局中常见的标签包括tr、th和td，表格行为tr（table row），表格数据为td（table data），表格表头th（table heading）。那么如何抓取这些标签之间的内容呢？下面代码是获取它们之间内容。 假设存在HTML代码如下所示： 表格 学号姓名 1001杨秀璋 1002严娜 则爬取对应值的Python代码如下： # coding=utf-8 import re import urllib content = urllib.urlopen(\"test.html\").read() #打开本地文件 #获取间内容 res = r'(.*?)' texts = re.findall(res, content, re.S|re.M) for m in texts: print m \"\"\" texts: ['学号姓名', '1001杨秀璋', '1002严娜'] \"\"\" #获取间内容 for m in texts: res_th = r'(.*?)' m_th = re.findall(res_th, m, re.S|re.M) for t in m_th: print t #直接获取间内容 res = r'(.*?)(.*?)' texts = re.findall(res, content, re.S|re.M) \"\"\" texts: [('1001', '杨秀璋'), ('1002', '严娜')] \"\"\" for m in texts: print m[0],m[1] 输出结果如下，首先获取tr之间的内容，然后再在tr之间内容中获取和之间值，即“学号”、“姓名”，最后讲述直接获取两个之间的内容方法。 >>> 学号姓名 1001杨秀璋 1002严娜 学号 姓名 1001 杨秀璋 1002 严娜 >>> 抓取标签中的参数 抓取超链接标签的URL HTML超链接的基本格式为链接内容，现在需要获取其中的URL链接地址，方法如下： # coding=utf-8 import re content = ''' 新闻 hao123 地图 视频 ''' res = r\"(? 输出内容如下： >>> http://news.baidu.com http://www.hao123.com http://map.baidu.com http://v.baidu.com >>> (? 解释 如果表达式为“(? (?=) 解释 如果表达式为“(?=.com)”，意思是：一个字符串后面跟着“.com”才做匹配操作，并不使用任何目标字符串 抓取图片超链接标签的URL HTML插入图片使用标签的基本格式为，则需要获取图片URL链接地址的方法如下： content = '''''' urls = re.findall('src=\"(.*?)\"', content, re.I|re.S|re.M) print urls # ['http://www..csdn.net/eastmount.jpg'] 其中图片对应的超链接为http://www..csdn.net/eastmount.jpg，这些资源通常存储在服务器端，最后一个“/”后面的字段即为资源的名称，该图片名称为“eastmount.jpg”。那么如何获取URL中最后一个参数呢？ 获取URL中最后一个参数 通常在使用Python爬取图片过程中，会遇到图片对应的URL最后一个字段通常用于命名图片，如前面的“eastmount.jpg”，需要通过URL“/”后面的参数获取图片。 content = '''''' urls = 'http://www..csdn.net/eastmount.jpg' name = urls.split('/')[-1] print name # eastmount.jpg 该段代码表示采用字符“/”分割字符串，并且获取最后一个获取的值，即为图片名称。 字符串处理及替换 在使用正则表达式爬取网页文本时，通常需要调用find() 函数找到指定的位置，再进行进一步爬取，比如获取class属性为“infobox”的表格table，再进行定位爬取。 start = content.find(r'') #重点点位置 infobox = text[start:end] print infobox 同时爬取过程中可能会爬取到无关变量，此时需要对无关内容进行过滤，这里推荐使用replace函数和正则表达式进行处理。比如，爬取内容如下： # coding=utf-8 import re content = ''' 1001杨秀璋 1002颜 娜 1003Python ''' res = r'(.*?)(.*?)' texts = re.findall(res, content, re.S|re.M) for m in texts: print m[0],m[1] 输出如下所示： >>> 1001 杨秀璋 1002 颜 娜 1003 Python >>> 此时需要过滤多余字符串，如换行（）、空格（ ）、加粗（）。 过滤代码如下： # coding=utf-8 import re content = ''' 1001杨秀璋 1002颜 娜 1003Python ''' res = r'(.*?)(.*?)' texts = re.findall(res, content, re.S|re.M) for m in texts: value0 = m[0].replace('', '').replace(' ', '') value1 = m[1].replace('', '').replace(' ', '') if '' in value1: m_value = re.findall(r'(.*?)', value1, re.S|re.M) print value0, m_value[0] else: print value0, value1 采用replace将字符串“”或“' ”替换成空白，实现过滤，而加粗（）需要使用正则表达式过滤，输出结果如下： >>> 1001 杨秀璋 1002 颜娜 1003 Python >>> 参考 [python爬虫] 正则表达式使用技巧及爬取个人博客实例 Update time： 2020-05-26 "},"Chapter1/识别文件的编码格式.html":{"url":"Chapter1/识别文件的编码格式.html","title":"识别文件的编码格式","keywords":"","body":"识别文件的编码格式 chardet库文档:https://chardet.readthedocs.io/en/latest/usage.html 模块介绍 Chardet：通用字符编码检测器 检测字符集范围： ASCII，UTF-8，UTF-16（2种变体），UTF-32（4种变体） Big5，GB2312，EUC-TW，HZ-GB-2312，ISO-2022-CN（繁体中文和简体中文） EUC-JP，SHIFT_JIS，CP932，ISO-2022-JP（日文） EUC-KR，ISO-2022-KR（韩文） KOI8-R，MacCyrillic，IBM855，IBM866，ISO-8859-5，windows-1251（西里尔文） ISO-8859-5，windows-1251（保加利亚语） ISO-8859-1，windows-1252（西欧语言） ISO-8859-7，windows-1253（希腊语） ISO-8859-8，windows-1255（视觉和逻辑希伯来语） TIS-620（泰国语）d'y 当python程序中某一个数据文件不知道编码时，可使用chardet第三方库来检测，代码如下（path中填对应文件路径即可) import chardet if __name__ == '__main__': path='***' f=open(path,'rb') data=f.read() print(chardet.detect(data)) # {'language': '', 'confidence': 0.73, 'encoding': 'Windows-1252'} detect函数只需要一个 非unicode字符串参数，返回一个字典。该字典包括判断到的编码格式及判断的置信度。 chardet.detect() 的返回值，为一个字典： {'language': '', 'confidence': 0.73, 'encoding': 'Windows-1252'} 得到文件的编码方式，可以才采用字典的方式 codedetect = chardet.detect(data)[\"encoding\"] #检测得到编码方式 将获取的内容进行解码 code = chardet.detect(response.content)[\"encoding\"] # 获取编码格式 response.encoding = code # 指定编码格式 return response.text Update time： 2020-05-26 "},"Chapter1/fake-useragent.html":{"url":"Chapter1/fake-useragent.html","title":"fake-useragent","keywords":"","body":"fake-useragent Python库(fake-useragent),可以随机生成各种UserAgent 安装 pip install fake-useragent 基本使用 from fake_useragent import UserAgent ua = UserAgent() from fake_useragent import UserAgent ua = UserAgent() ua.ie # Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US); ua.msie # Mozilla/5.0 (compatible; MSIE 10.0; Macintosh; Intel Mac OS X 10_7_3; Trident/6.0)' ua['Internet Explorer'] # Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.1; Trident/4.0; GTB7.4; InfoPath.2; SV1; .NET CLR 3.3.69573; WOW64; en-US) ua.opera # Opera/9.80 (X11; Linux i686; U; ru) Presto/2.8.131 Version/11.11 ua.chrome # Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.2 (KHTML, like Gecko) Chrome/22.0.1216.0 Safari/537.2' ua.google # Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_4) AppleWebKit/537.13 (KHTML, like Gecko) Chrome/24.0.1290.1 Safari/537.13 ua['google chrome'] # Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11 ua.firefox # Mozilla/5.0 (Windows NT 6.2; Win64; x64; rv:16.0.1) Gecko/20121011 Firefox/16.0.1 ua.ff # Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:15.0) Gecko/20100101 Firefox/15.0.1 ua.safari # Mozilla/5.0 (iPad; CPU OS 6_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/6.0 Mobile/10A5355d Safari/8536.25 # and the best one, random via real world browser usage statistic ua.random 注意: fake-useragent 将收集到的数据缓存到temp文件夹, 例如 /tmp, 更新数据: from fake_useragent import UserAgent ua = UserAgent() ua.update() 有时候会因为网络或者其他问题,出现异常(fake_useragent.errors.FakeUserAgentError: Maximum amount of retries reached), 可以禁用服务器缓存 from fake_useragent import UserAgent ua = UserAgent(use_cache_server=False) 可以自己添加本地数据文件 import fake_useragent # I am STRONGLY!!! recommend to use version suffix location = '/home/user/fake_useragent%s.json' % fake_useragent.VERSION ua = fake_useragent.UserAgent(path=location) ua.random 参考 fake-useragent 0.1.11 Update time： 2020-05-26 "},"Chapter1/AJAX动态网页数据抓取、Selenium使用.html":{"url":"Chapter1/AJAX动态网页数据抓取、Selenium使用.html","title":"AJAX动态网页数据抓取、Selenium使用","keywords":"","body":"AJAX动态网页数据抓取、Selenium使用 什么是AJAX AJAX（Asynchronouse JavaScript And XML）异步JavaScript和XML。过在后台与服务器进行少量数据交换，Ajax 可以使网页实现异步更新。这意味着可以在不重新加载整个网页的情况下，对网页的某部分进行更新。传统的网页（不使用Ajax）如果需要更新内容，必须重载整个网页页面。因为传统的在传输数据格式方面，使用的是XML语法。因此叫做AJAX，其实现在数据交互基本上都是使用JSON。使用AJAX加载的数据，即使使用了JS，将数据渲染到了浏览器中，在右键->查看网页源代码还是不能看到通过ajax加载的数据，只能看到使用这个url加载的html代码。 获取ajax数据的方式 直接分析ajax调用的接口。然后通过代码请求这个接口。 使用Selenium+chromedriver模拟浏览器行为获取数据。 接口分析 豆瓣电影爬取 https://movie.douban.com/typerank?type_name=%E5%96%9C%E5%89%A7&type=24& interval_id=100:90&action= 打开页面，电影页面没有分页显示按钮，当鼠标向下滚动时，会加载新的电影， 查看网页，查看Request URL:分析 https://movie.douban.com/j/chart/top_list?type=24& interval_id=100%3A90&action=&start=0&limit=20 URL中的 start 参数是动态变化的 测试，能获取‘一页’的电影 import requests url = \"https://movie.douban.com/j/chart/top_list?type=24& interval_id=100%3A90&action=&start=20&limit=20\" res = requests.get(url) print(res.text) 测试，获取所有电影 import requests base_url = \"https://movie.douban.com/j/chart/top_list?type=24& interval_id=100%3A90&action=&start={}&limit=20\" #可以适当的增加 limit= 的值 提高效率 i = 0 while True: print(i) url=base_url.format(i*20) res=requests.get(url) info = res.text print(len(info)) if len(info) == 0: break i += 1 Selenium+chromedriver获取动态数据 Selenium 相当于是一个机器人。可以模拟人类在浏览器上的一些行为，自动处理浏览器上的一些行为，比如点击，填充数据，删除cookie等。chromedriver是一个驱动Chrome浏览器的驱动程序，使用他才可以驱动浏览器。当然针对不同的浏览器有不同的driver。以下列出了不同浏览器及其对应的driver： Chrome Firefox Edge Safari 安装Selenium和chromedriver 安装Selenium：Selenium有很多语言的版本，有java、ruby、python等。我们下载python版本的就可以了 pip install selenium 安装chromedriver：下载完成后，放到不需要权限的纯英文目录下就可以了。 快速入门 现在以一个简单的获取百度首页的例子来讲下Selenium和chromedriver如何快速入门： from selenium import webdriver # chromedriver的绝对路径 driver_path = r'D:\\ProgramApp\\chromedriver\\chromedriver.exe' # 初始化一个driver，并且指定chromedriver的路径 driver = webdriver.Chrome(executable_path=driver_path) # 请求网页 driver.get(\"https://www.baidu.com/\") # 通过page_source获取网页代码（浏览器渲染完后的代码） print(driver.page_source) selenium常用操作 更多教程请参考 定位元素方法 Selenium提供了一下方法来定义一个页面中的元素： find_element_by_id find_element_by_name find_element_by_xpath find_element_by_link_text find_element_by_partial_link_text find_element_by_tag_name find_element_by_class_name find_element_by_css_selector 下面是查找多个元素（这些方法将返回一个列表） find_elements_by_name find_elements_by_xpath find_elements_by_link_text find_elements_by_partial_link_text find_elements_by_tag_name find_elements_by_class_name find_elements_by_css_selector 常用方法是通过xpath相对路径进行定位, 例如 定位username元素的方法如下： username = driver.find_element_by_xpath(\"//form[input/@name='username']\") username = driver.find_element_by_xpath(\"//form[@id='loginForm']/input[1]\") username = driver.find_element_by_xpath(\"//input[@name='username']\") [1] 第一个form元素通过一个input子元素，name属性和值为username实现 [2] 通过id=loginForm值的form元素找到第一个input子元素 [3] 属性名为name且值为username的第一个input元素 操作元素方法 通常所有的操作与页面交互都将通过WebElement接口，常见的操作元素方法如下： clear 清除元素的内容 send_keys 模拟按键输入 click 点击元素 submit 提交表单 举例自动访问FireFox浏览器自动登录163邮箱。 from selenium import webdriver from selenium.webdriver.common.keys import Keys import time # Login 163 email driver = webdriver.Firefox() driver.get(\"http://mail.163.com/\") elem_user = driver.find_element_by_name(\"username\") elem_user.clear elem_user.send_keys(\"15201615157\") elem_pwd = driver.find_element_by_name(\"password\") elem_pwd.clear elem_pwd.send_keys(\"******\") elem_pwd.send_keys(Keys.RETURN) #driver.find_element_by_id(\"loginBtn\").click() #driver.find_element_by_id(\"loginBtn\").submit() time.sleep(5) assert \"baidu\" in driver.title driver.close() driver.quit() 首先通过name定位用户名和密码，再调用方法clear()清除输入框默认内容，如“请输入密码”等提示，通过send_keys(\"**\")输入正确的用户名和密码，最后通过click()点击登录按钮或send_keys(Keys.RETURN)相当于回车登录，submit()提交表单。 PS：如果需要输入中文，防止编码错误使用send_keys(u\"中文用户名\")。 WebElement接口获取值 通过WebElement接口可以获取常用的值，这些值同样非常重要。 size 获取元素的尺寸 text 获取元素的文本 get_attribute(name) 获取属性值 location 获取元素坐标，先找到要获取的元素，再调用该方法 page_source 返回页面源码 driver.title 返回页面标题 current_url 获取当前页面的URL is_displayed() 设置该元素是否可见 is_enabled() 判断元素是否被使用 is_selected() 判断元素是否被选中 tag_name 返回元素的tagName from selenium import webdriver from selenium.webdriver.common.keys import Keys import time driver = webdriver.PhantomJS(executable_path=\"G:\\phantomjs-1.9.1-windows\\phantomjs.exe\") driver.get(\"http://www.baidu.com/\") size = driver.find_element_by_name(\"wd\").size print size #尺寸: {'width': 500, 'height': 22} news = driver.find_element_by_xpath(\"//div[@id='u1']/a[1]\").text print news #文本: 新闻 href = driver.find_element_by_xpath(\"//div[@id='u1']/a[2]\").get_attribute('href') name = driver.find_element_by_xpath(\"//div[@id='u1']/a[2]\").get_attribute('name') print href,name #属性值: http://www.hao123.com/ tj_trhao123 location = driver.find_element_by_xpath(\"//div[@id='u1']/a[3]\").location print location #坐标: {'y': 19, 'x': 498} print driver.current_url #当前链接: https://www.baidu.com/ print driver.title #标题: 百度一下， 你就知道 result = location = driver.find_element_by_id(\"su\").is_displayed() print result #是否可见: True 鼠标操作 在现实的自动化测试中关于鼠标的操作不仅仅是click()单击操作，还有很多包含在ActionChains类中的操作。如下： context_click(elem) 右击鼠标点击元素elem，另存为等行为 double_click(elem) 双击鼠标点击元素elem，地图web可实现放大功能 drag_and_drop(source,target) 拖动鼠标，源元素按下左键移动至目标元素释放 move_to_element(elem) 鼠标移动到一个元素上 click_and_hold(elem) 按下鼠标左键在一个元素上 perform() 在通过调用该函数执行ActionChains中存储行为 关闭浏览器 driver.close()：关闭当前页面。 driver.quit()：退出整个浏览器。 方法 区别 close 关闭当前的浏览器窗口 quit 不仅关闭窗口，还会彻底的退出webdriver，释放与driver server之间的连接 页面等待 现在的网页越来越多采用了 Ajax 技术，这样程序便不能确定何时某个元素完全加载出来了。如果实际页面等待时间过长导致某个dom元素还没出来，但是你的代码直接使用了这个WebElement，那么就会抛出NullPointer的异常。为了解决这个问题。所以 Selenium 提供了两种等待方式：一种是隐式等待、一种是显式等待。 隐式等待：调用driver.implicitly_wait。那么在获取不可用的元素之前，会先等待10秒中的时间。示例代码如下： driver = webdriver.Chrome(executable_path=driver_path) driver.implicitly_wait(10) # 请求网页 driver.get(\"https://www.douban.com/\") selenium_通过selenium控制浏览器滚动条 目的：通过selenium控制浏览器滚动条 原理：通过 driver.execute_script()执行js代码，达到目的 driver.execute_script(\"window.scrollBy(0,1000)\") 语法：scrollBy(x,y) 参数 描述 x 必需。向右滚动的像素值。 y 必需。向下滚动的像素值。 或者使用 js=\"var q=document.documentElement.scrollTop=10000\" driver.execute_script(js) 例如在京东的某个页面，当鼠标向下滚动的时候才会把整个页面的商品加载出来，利用 driver.execute_script() 可以设置某个阈值将页面一次滑倒最底端 from selenium import webdriver from lxml import etree from time import sleep url = 'https://search.jd.com/Search?keyword=mac&enc=utf-8&wq=mac&pvid=9862d03c24e741c6a58079d004f5aabf' chrome = webdriver.Chrome() chrome.get(url) js = 'document.documentElement.scrollTop=100000' chrome.execute_script(js) sleep(3) html = chrome.page_source e = etree.HTML(html) prices = e.xpath('//div[@class=\"gl-i-wrap\"]/div[@class=\"p-price\"]/strong/i/text()') names = e.xpath('//div[@class=\"gl-i-wrap\"]/div[@class=\"p-name p-name-type-2\"]/a/em') print(len(names)) for name, price in zip(names, prices): print(name.xpath('string(.)'), \":\", price) chrome.quit() selenium操作无界面chrome浏览器 from selenium import webdriver from selenium.webdriver.chrome.options import Options req_url = \"https://www.baidu.com\" chrome_options=Options() #设置chrome浏览器无界面模式 chrome_options.add_argument('--headless') browser = webdriver.Chrome(chrome_options=chrome_options) # 开始请求 browser.get(req_url) #打印页面源代码 print(browser.page_source) #关闭浏览器 browser.close() #关闭chreomedriver进程 browser.quit() selenium爬取局部动态刷新网站（URL始终固定） 测试，虎牙直播，当进入某个直播分类的时候，点击不同的分页，URL不发生变化，可以通过selenium的click()事件，实现翻页的情况， from selenium import webdriver from lxml import etree import time import requests driver_path=r'D:\\chromedriver_win32\\chromedriver.exe' driver = webdriver.Chrome(executable_path=driver_path) content=[] url = 'https://www.huya.com/g/wzry' driver.get(url) driver.implicitly_wait(1) m=1 def page(): global m global content while m 参考 [python爬虫] Selenium常见元素定位方法和操作的学习介绍 Selenium-Python中文文档 Update time： 2020-05-26 "},"Scrapy/":{"url":"Scrapy/","title":"Scrapy","keywords":"","body":"Scrapy Update time： 2020-05-27 "},"Scrapy/Scrapy介绍.html":{"url":"Scrapy/Scrapy介绍.html","title":"Scrapy介绍","keywords":"","body":"Scrapy介绍 写一个爬虫，需要做很多的事情。比如：发送网络请求、数据解析、数据存储、反反爬虫机制（更换ip代理、设置请求头等）、异步请求等。这些工作如果每次都要自己从零开始写的话，比较浪费时间。因此Scrapy 把一些基础的东西封装好了，在他上面写爬虫可以变的更加的高效（爬取效率和开发效率）。因此真正在公司里，一些上了量的爬虫，都是使用Scrapy框架来解决。 Scrapy框架模块功能 Scrapy Engine（引擎）： Scrapy框架的核心部分。负责在Spider和 ItemPipeline、Downloader、Scheduler中间通信、传递数据等。 Spider（爬虫）：发送需要爬取的链接给引擎，最后引擎把其他模块请求回来的数据再发送给爬虫，爬虫就去解析想要的数据。这个部分是我们开发者自己写的，因为要爬取哪些链接，页面中的哪些数据是我们需要的，都是由程序员自己决定。 Scheduler（调度器）：负责接收引擎发送过来的请求，并按照一定的方式进行排列和整理，负责调度请求的顺序等。 Downloader（下载器）：负责接收引擎传过来的下载请求，然后去网络上下载对应的数据再交还给引擎。 Item Pipeline（管道）：负责将Spider（爬虫）传递过来的数据进行保存。具体保存在哪里，应该看开发者自己的需求。 Downloader Middlewares（下载中间件）：可以扩展下载器和引擎之间通信功能的中间件。 Spider Middlewares（Spider中间件）：可以扩展引擎和爬虫之间通信功能的中间件。 安装和文档 安装 pip install scrapy \"\"\" 注意：如果在windows系统下，提示这个错误 ModuleNotFoundError: No module named ‘win32api’， 那么使用以下命令可以解决：pip install pypiwin32。 \"\"\" Scrapy官方文档 Scrapy中文文档 创建项目 要使用Scrapy框架创建项目，需要通过命令来创建。首先进入到你想把这个项目存放的目录。然后使用以下命令创建： #scrapy startproject [项目名称] scrapy startproject qss 目录结构介绍： 以下介绍下主要文件的作用： items.py：用来存放爬虫爬取下来数据的模型。 middlewares.py：用来存放各种中间件的文件。 pipelines.py：用来将items的模型存储到本地磁盘中。 settings.py：本爬虫的一些配置信息（比如请求头、多久发送一次请求、ip代理池等）。 scrapy.cfg：项目的配置文件。 spiders包：以后所有的爬虫，都是存放到这个里面。 定义Item Item 是保存爬取到的数据的容器；其使用方法和python字典类似， 并且提供了额外保护机制来避免拼写错误导致的未定义字段错误。 可以通过创建一个 scrapy.Item 类， 并且定义类型为 scrapy.Field 的类属性来定义一个Item。 import scrapy class DmozItem(scrapy.Item): title = scrapy.Field() link = scrapy.Field() desc = scrapy.Field() 编写第一个爬虫(Spider) Spider是用户编写用于从单个网站(或者一些网站)爬取数据的类 其包含了一个用于下载的初始URL，如何跟进网页中的链接以及如何分析页面中的内容， 提取生成 item 的方法。 为了创建一个Spider，您必须继承 scrapy.Spider 类， 且定义以下三个属性: name: 用于区别Spider。 该名字必须是唯一的，您不可以为不同的Spider设定相同的名字。 start_urls: 包含了Spider在启动时进行爬取的url列表。 因此，第一个被获取到的页面将是其中之一。 后续的URL则从初始的URL获取到的数据中提取。 parse() 是spider的一个方法。 被调用时，每个初始URL完成下载后生成的 Response 对象将会作为唯一的参数传递给该函数。 该方法负责解析返回的数据(response data)，提取数据(生成item)以及生成需要进一步处理的URL的 Request 对象。 使用命令创建一个爬虫 首先进入项目的文件夹 cd spider scrapy genspider qsbk \"qiushibaike.com\" 创建了一个名字叫做 qsbk 的爬虫，并且能爬取的网页只会限制在 qiushibaike.com 这个域名下。 创建项目和爬虫： 创建项目：scrapy startproject [爬虫的名字]。 创建爬虫：进入到项目所在的路径，执行命令：scrapy genspider [爬虫名字] [爬虫的域名]。注意，爬虫名字不能和项目名称一致。 运行scrapy项目 进入项目的根目录，执行下列命令启动spider: # scrapy crawl [爬虫名字] scrapy crawl qsbk 如果不想每次都在命令行中运行，那么可以把这个命令写在一个文件中。以后就在pycharm中执行运行这个文件就可以了。比如现在新创建一个文件叫做start.py，然后在这个文件中填入以下代码： from scrapy import cmdline cmdline.execute(\"scrapy crawl qsbk\".split()) Update time： 2020-05-27 "},"Scrapy/Items.html":{"url":"Scrapy/Items.html","title":"Items","keywords":"","body":"Items 爬取的主要目标就是从非结构性的数据源提取结构性数据，例如网页。 Scrapy提供 Item 类来满足这样的需求。 Item 对象是种简单的容器，保存了爬取到得数据。 其提供了 类似于词典(dictionary-like) 的API以及用于声明可用字段的简单语法。 声明Item Item使用简单的class定义语法以及 Field 对象来声明。例如: import scrapy class Product(scrapy.Item): name = scrapy.Field() price = scrapy.Field() stock = scrapy.Field() last_updated = scrapy.Field(serializer=str) 创建item >>> product = Product(name='Desktop PC', price=1000) >>> print product Product(name='Desktop PC', price=1000) 获取字段的值 >>> product['name'] Desktop PC >>> product.get('name') Desktop PC >>> product['price'] 1000 设置字段的值 >>> product['last_updated'] = 'today' >>> product['last_updated'] today 获取所有获取到的值 >>> product.keys() ['price', 'name'] >>> product.items() [('price', 1000), ('name', 'Desktop PC')] Update time： 2020-05-27 "},"Scrapy/Spiders.html":{"url":"Scrapy/Spiders.html","title":"Spiders","keywords":"","body":"Spiders Spider类定义了如何爬取某个(或某些)网站。包括了爬取的动作(例如:是否跟进链接)以及如何从网页的内容中提取结构化数据(爬取item)。 换句话说，Spider就是您定义爬取的动作及分析某个网页(或者是有些网页)的地方。 对spider来说，爬取的循环类似下文: 以初始的URL初始化Request，并设置回调函数。 当该request下载完毕并返回时，将生成response，并作为参数传给该回调函数。 spider中初始的request是通过调用 start_requests() 来获取的。 start_requests() 读取 start_urls 中的URL， 并以 parse 为回调函数生成 Request 。 在回调函数内分析返回的(网页)内容，返回 Item 对象或者 Request 或者一个包括二者的可迭代容器。 返回的Request对象之后会经过Scrapy处理，下载相应的内容，并调用设置的callback函数(函数可相同)。 在回调函数内，您可以使用 选择器(Selectors) (您也可以使用BeautifulSoup, lxml 或者您想用的任何解析器) 来分析网页内容，并根据分析的数据生成item。 最后，由spider返回的item将被存到数据库(由某些 Item Pipeline 处理)或使用 Feed exports 存入到文件中。 Spider classscrapy.spider.Spider Spider是最简单的spider。每个其他的spider必须继承自该类(包括Scrapy自带的其他spider以及您自己编写的spider)。 Spider并没有提供什么特殊的功能。 其仅仅请求给定的 start_urls/start_requests ，并根据返回的结果(resulting responses)调用spider的 parse 方法。 name 定义spider名字的字符串(string)。spider的名字定义了Scrapy如何定位(并初始化)spider，所以其必须是唯一的。 不过您可以生成多个相同的spider实例(instance)，这没有任何限制。 name是spider最重要的属性，而且是必须的。 allowed_domains 可选。包含了spider允许爬取的域名(domain)列表(list)。 当 OffsiteMiddleware 启用时， 域名不在列表中的URL不会被跟进。 start_urls URL列表。当没有制定特定的URL时，spider将从该列表中开始进行爬取。 因此，第一个被获取到的页面的URL将是该列表之一。 后续的URL将会从获取到的数据中提取。 parse(response) 当response没有指定回调函数时，该方法是 Scrapy 处理下载的response的默认方法。 parse 负责处理response并返回处理的数据以及(/或)跟进的URL。 Spider 对其他的Request的回调函数也有相同的要求。 该方法及其他的Request回调函数必须返回一个包含 Request 及(或) Item 的可迭代的对象。 参数: response (Response) – 用于分析的response ，相当于request 库的返回值 可以执行 xpath 语法 Spider样例 import scrapy class MySpider(scrapy.Spider): name = 'example.com' allowed_domains = ['example.com'] start_urls = [ 'http://www.example.com/1.html', 'http://www.example.com/2.html', 'http://www.example.com/3.html', ] def parse(self, response): self.log('A response from %s just arrived!' % response.url) 另一个在单个回调函数中返回多个Request以及Item的例子: import scrapy from myproject.items import MyItem class MySpider(scrapy.Spider): name = 'example.com' allowed_domains = ['example.com'] start_urls = [ 'http://www.example.com/1.html', 'http://www.example.com/2.html', 'http://www.example.com/3.html', ] def parse(self, response): sel = scrapy.Selector(response) for h3 in response.xpath('//h3').extract(): yield MyItem(title=h3) for url in response.xpath('//a/@href').extract(): yield scrapy.Request(url, callback=self.parse) CrawlSpider classscrapy.contrib.spiders.CrawlSpider 爬取一般网站常用的spider。其定义了一些规则(rule)来提供跟进link的方便的机制。 也许该spider并不是完全适合您的特定网站或项目，但其对很多情况都使用。 因此您可以以其为起点，根据需求修改部分方法。当然您也可以实现自己的spider。 除了从Spider继承过来的(您必须提供的)属性外，其提供了一个新的属性: rules 一个包含一个(或多个) Rule 对象的集合(list)。 每个 Rule 对爬取网站的动作定义了特定表现。 Rule对象在下边会介绍。 如果多个rule匹配了相同的链接，则根据他们在本属性中被定义的顺序，第一个会被使用。 parse_start_url(response) 当start_url的请求返回时，该方法被调用。 该方法分析最初的返回值并必须返回一个 Item 对象或者 一个 Request 对象或者 一个可迭代的包含二者对象。 爬取规则(Crawling rules) classscrapy.contrib.spiders.Rule(link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None) link_extractor 是一个 Link Extractor 对象。 其定义了如何从爬取到的页面提取链接。 callback 是一个callable或string(该spider中同名的函数将会被调用)。 从link_extractor中每获取到链接时将会调用该函数。该回调函数接受一个response作为其第一个参数， 并返回一个包含 Item 以及(或) Request 对象(或者这两者的子类)的列表(list)。 cb_kwargs 包含传递给回调函数的参数(keyword argument)的字典。 follow 是一个布尔(boolean)值，指定了根据该规则从response提取的链接是否需要跟进。 如果 callback 为None， follow 默认设置为 True ，否则默认为 False 。 process_links 是一个callable或string(该spider中同名的函数将会被调用)。 从link_extractor中获取到链接列表时将会调用该函数。该方法主要用来过滤。 process_request 是一个callable或string(该spider中同名的函数将会被调用)。 该规则提取到每个request时都会调用该函数。该函数必须返回一个request或者None。 (用来过滤request) CrawlSpider样例 import scrapy from scrapy.contrib.spiders import CrawlSpider, Rule from scrapy.contrib.linkextractors import LinkExtractor class MySpider(CrawlSpider): name = 'example.com' allowed_domains = ['example.com'] start_urls = ['http://www.example.com'] rules = ( # 提取匹配 'category.php' (但不匹配 'subsection.php') 的链接并跟进链接(没有callback意味着follow默认为True) Rule(LinkExtractor(allow=('category\\.php', ), deny=('subsection\\.php', ))), # 提取匹配 'item.php' 的链接并使用spider的parse_item方法进行分析 Rule(LinkExtractor(allow=('item\\.php', )), callback='parse_item'), ) def parse_item(self, response): self.log('Hi, this is an item page! %s' % response.url) item = scrapy.Item() item['id'] = response.xpath('//td[@id=\"item_id\"]/text()').re(r'ID: (\\d+)') item['name'] = response.xpath('//td[@id=\"item_name\"]/text()').extract() item['description'] = response.xpath('//td[@id=\"item_description\"]/text()').extract() return item 该spider将从example.com的首页开始爬取，获取category以及item的链接并对后者使用 parse_item 方法。 当item获得返回(response)时，将使用XPath处理HTML并生成一些数据填入 Item 中。 Update time： 2020-05-27 "},"Scrapy/Item Pipeline.html":{"url":"Scrapy/Item Pipeline.html","title":"Item Pipeline","keywords":"","body":"Item Pipeline 当Item在Spider中被收集之后，它将会被传递到Item Pipeline，一些组件会按照一定的顺序执行对Item的处理。 每个item pipeline组件(有时称之为“Item Pipeline”)是实现了简单方法的Python类。他们接收到Item并通过它执行一些行为，同时也决定此Item是否继续通过pipeline，或是被丢弃而不再进行处理。 以下是item pipeline的一些典型应用： 清理HTML数据 验证爬取的数据(检查item包含某些字段) 查重(并丢弃) 将爬取结果保存到数据库中 编写你自己的item pipeline 编写你自己的item pipeline很简单，每个item pipeline组件是一个独立的Python类，同时必须实现以下方法: process_item(self, item**, spider) 每个item pipeline组件都需要调用该方法，这个方法必须返回一个 Item** (或任何继承类)对象， 或是抛出 DropItem 异常，被丢弃的item将不会被之后的pipeline组件所处理。 参数: item (Item 对象) – 被爬取的item spider (Spider 对象) – 爬取该item的spider 此外,他们也可以实现以下方法: open_spider(self, spider) 当spider被开启时，这个方法被调用。 参数: spider (Spider 对象) – 被开启的spider close_spider(spider) 当spider被关闭时，这个方法被调用 参数: spider (Spider 对象) – 被关闭的spider Item pipeline 样例 验证价格，同时丢弃没有价格的item 让我们来看一下以下这个假设的pipeline，它为那些不含税(price_excludes_vat 属性)的item调整了 price 属性，同时丢弃了那些没有价格的item: from scrapy.exceptions import DropItem class PricePipeline(object): vat_factor = 1.15 def process_item(self, item, spider): if item['price']: if item['price_excludes_vat']: item['price'] = item['price'] * self.vat_factor return item else: raise DropItem(\"Missing price in %s\" % item) 将item写入JSON文件 以下pipeline将所有(从所有spider中)爬取到的item，存储到一个独立地 items.jl 文件，每行包含一个序列化为JSON格式的item: import json class JsonWriterPipeline(object): def __init__(self): self.file = open('items.jl', 'wb') def process_item(self, item, spider): line = json.dumps(dict(item)) + \"\\n\" self.file.write(line) return item Write items to MongoDB import pymongo class MongoPipeline(object): def __init__(self, mongo_uri, mongo_db): self.mongo_uri = mongo_uri self.mongo_db = mongo_db @classmethod def from_crawler(cls, crawler): return cls( mongo_uri=crawler.settings.get('MONGO_URI'), mongo_db=crawler.settings.get('MONGO_DATABASE', 'items') ) def open_spider(self, spider): self.client = pymongo.MongoClient(self.mongo_uri) self.db = self.client[self.mongo_db] def close_spider(self, spider): self.client.close() def process_item(self, item, spider): collection_name = item.__class__.__name__ self.db[collection_name].insert(dict(item)) return item 启用一个Item Pipeline组件 ITEM_PIPELINES = { 'myproject.pipelines.PricePipeline': 300, 'myproject.pipelines.JsonWriterPipeline': 800, } 分配给每个类的整型值，确定了他们运行的顺序，item按数字从低到高的顺序，通过pipeline，通常将这些数字定义在0-1000范围内。 pipeline区分传来Items 各个页面都会封装items并将item传递给pipelines来处理，而pipelines接收的入口只有一个就是 def process_item(self, item, spider): pass spider对应相应的爬虫，调用spider.name也可区分来自不同爬虫的item def process_item(self, item, spider): if spider.name == \"XXXX\": pass Update time： 2020-05-27 "},"Scrapy/下载项目图片.html":{"url":"Scrapy/下载项目图片.html","title":"下载项目图片","keywords":"","body":"下载项目图片 Scrapy提供了一个 item pipeline ，来下载属于某个特定项目的图片，比如，当你抓取产品时，也想把它们的图片下载到本地。 这条管道，被称作图片管道，在 ImagesPipeline 类中实现，提供了一个方便并具有额外特性的方法，来下载并本地存储图片: 将所有下载的图片转换成通用的格式（JPG）和模式（RGB） 避免重新下载最近已经下载过的图片 缩略图生成 检测图像的宽/高，确保它们满足最小限制 这个管道也会为那些当前安排好要下载的图片保留一个内部队列，并将那些到达的包含相同图片的项目连接到那个队列中。 这可以避免多次下载几个项目共享的同一个图片。 使用图片管道 当使用 ImagesPipeline ，典型的工作流程如下所示: 在一个爬虫里，你抓取一个项目，把其中图片的URL放入 image_urls 组内。 项目从爬虫内返回，进入项目管道。 当项目进入 ImagesPipeline，image_urls 组内的URLs将被Scrapy的调度器和下载器（这意味着调度器和下载器的中间件可以复用）安排下载，当优先级更高，会在其他页面被抓取前处理。项目会在这个特定的管道阶段保持“locker”的状态，直到完成图片的下载（或者由于某些原因未完成下载）。 当图片下载完，另一个组(images)将被更新到结构中。这个组将包含一个字典列表，其中包括下载图片的信息，比如下载路径、源抓取地址（从 image_urls 组获得）和图片的校验码。 images 列表中的图片顺序将和源 image_urls 组保持一致。如果某个图片下载失败，将会记录下错误信息，图片也不会出现在 images 组中。 使用样例 为了使用图片管道，你仅需要 启动它 并用 image_urls 和 images 定义一个项目: import scrapy class MyItem(scrapy.Item): # ... other item fields ... image_urls = scrapy.Field() images = scrapy.Field() 如果你需要更加复杂的功能，想重写定制图片管道行为，参见 实现定制图片管道 。 开启你的图片管道 为了开启你的图片管道，你首先需要在项目中添加它 ITEM_PIPELINES setting: ITEM_PIPELINES = {'scrapy.contrib.pipeline.images.ImagesPipeline': 1} 并将 IMAGES_STORE 设置为一个有效的文件夹，用来存储下载的图片。否则管道将保持禁用状态，即使你在 ITEM_PIPELINES 设置中添加了它。 比如: IMAGES_STORE = '/path/to/valid/dir' 图片存储 文件系统存储 图片存储在文件中（一个图片一个文件），并使用它们URL的 SHA1 hash 作为文件名。 比如，对下面的图片URL: http://www.example.com/image.jpg 它的 SHA1 hash 值为: 3afec3b4765f8f0a07b78f98c07b83f013567a0a 将被下载并存为下面的文件: /full/3afec3b4765f8f0a07b78f98c07b83f013567a0a.jpg 实现定制图片管道 下面是你可以在定制的图片管道里重写的方法： classscrapy.contrib.pipeline.images.ImagesPipeline get_media_requests(item, info**)** 在工作流程中可以看到，管道会得到图片的URL并从项目中下载。为了这么做，你需要重写 get_media_requests() 方法，并对各个图片URL返回一个Request: def get_media_requests(self, item, info): for image_url in item['image_urls']: yield scrapy.Request(image_url) 这些请求将被管道处理，当它们完成下载后，结果将以2-元素的元组列表形式传送到 item_completed() 方法: 默认 get_media_requests() 方法返回 None ，这意味着项目中没有图片可下载。 item_completed(results, items**, info)** 当一个单独项目中的所有图片请求完成时（要么完成下载，要么因为某种原因下载失败）， ImagesPipeline.item_completed() 方法将被调用。 item_completed() 方法需要返回一个输出，其将被送到随后的项目管道阶段，因此你需要返回（或者丢弃）项目，如你在任意管道里所做的一样。 这里是一个 item_completed() 方法的例子，其中我们将下载的图片路径（传入到results中）存储到 image_paths 项目组中，如果其中没有图片，我们将丢弃项目: from scrapy.exceptions import DropItem def item_completed(self, results, item, info): image_paths = [x['path'] for ok, x in results if ok] if not image_paths: raise DropItem(\"Item contains no images\") item['image_paths'] = image_paths return item 默认情况下， item_completed() 方法返回项目。 定制图片管道的例子 import scrapy from scrapy.contrib.pipeline.images import ImagesPipeline from scrapy.exceptions import DropItem class MyImagesPipeline(ImagesPipeline): def get_media_requests(self, item, info): for image_url in item['image_urls']: yield scrapy.Request(image_url) def item_completed(self, results, item, info): image_paths = [x['path'] for ok, x in results if ok] if not image_paths: raise DropItem(\"Item contains no images\") item['image_paths'] = image_paths return item Update time： 2020-05-27 "},"Scrapy/Scrapy的Request和Response.html":{"url":"Scrapy/Scrapy的Request和Response.html","title":"Scrapy的Request和Response","keywords":"","body":"Scrapy的Request和Response 请求和响应 Scrapy的Request 和Response对象用于爬网网站。 通常，Request对象在爬虫程序中生成并传递到系统，直到它们到达下载程序，后者执行请求并返回一个Response对象，该对象返回到发出请求的爬虫程序。 两个类Request和Response类都有一些子类，它们添加基类中不需要的功能。这些在下面的请求子类和 响应子类中描述。 class scrapy.http.Request(url[, callback, method='GET', headers, body, cookies, meta, encoding='utf-8', priority=0, dont_filter=False, errback] ) 一个Request对象表示一个HTTP请求，它通常是在爬虫生成，并由下载执行，从而生成Response。 常用参数： url（string） - 此请求的网址 callback（callable） - 将使用此请求的响应（一旦下载）作为其第一个参数调用的函数。有关更多信息，请参阅下面的将附加数据传递给回调函数。如果请求没有指定回调，parse()将使用spider的 方法。请注意，如果在处理期间引发异常，则会调用errback。 method（string） - 此请求的HTTP方法。默认为’GET’。 meta（dict） - 属性的初始值Request.meta。如果给定，在此参数中传递的dict将被浅复制。 headers（dict） - 这个请求的头。dict值可以是字符串（对于单值标头）或列表（对于多值标头）。如果 None作为值传递，则不会发送HTTP头。 body（str或unicode） - 请求体。如果unicode传递了a，那么它被编码为 str使用传递的编码（默认为utf-8）。如果 body没有给出，则存储一个空字符串。不管这个参数的类型，存储的最终值将是一个str（不会是unicode或None）。 cookie（dict或list） - 请求cookie。这些可以以两种形式发送。 dont_filter（boolean） - 表示此请求不应由调度程序过滤。当您想要多次执行相同的请求时忽略重复过滤器时使用。小心使用它，或者你会进入爬行循环。默认为False。 priority（int） - 此请求的优先级（默认为0）。调度器使用优先级来定义用于处理请求的顺序。具有较高优先级值的请求将较早执行。允许负值以指示相对低优先级。 encoding（string） - 此请求的编码（默认为’utf-8’）。此编码将用于对URL进行百分比编码，并将正文转换为str（如果给定unicode）。 Request中meta参数的作用是传递信息给下一个函数，使用过程可以理解成： 把需要传递的信息赋值给这个叫meta的变量， 但meta只接受字典类型的赋值，因此 要把待传递的信息改成“字典”的形式，即： meta={'key1':value1,'key2':value2} 如果想在下一个函数中取出value1, 只需得到上一个函数的meta['key1']即可， 因为meta是随着Request产生时传递的， 下一个函数得到的Response对象中就会有meta， 即response.meta， 取value1则是value1=response.meta['key1'] class example(scrapy.Spider): name='example' allowed_domains=['example.com'] start_urls=['http://www.example.com'] def parse(self,response): #从start_urls中分析出的一个网址赋值给url url=response.xpath('.......').extract() #ExamleClass是在items.py中定义的,下面会写出。 \"\"\"记住item本身是一个字典\"\"\" item=ExampleClass() item['name']=response.xpath('.......').extract() item['htmlurl']=response.xpath('.......').extract() \"\"\"通过meta参数，把item这个字典，赋值给meta中的'key'键（记住meta本身也是一个字典）。 Scrapy.Request请求url后生成一个\"Request对象\"，这个meta字典（含有键值'key'，'key'的值也是一个字典，即item） 会被“放”在\"Request对象\"里一起发送给parse2()函数 \"\"\" yield Request(url,meta={'key':item},callback='parse2') def parse2(self,response): item=response.meta['key'] \"\"\"这个response已含有上述meta字典，此句将这个字典赋值给item， 完成信息传递。这个item已经和parse中的item一样了\"\"\" item['text']=response.xpath('.......').extract() #item共三个键值，到这里全部添加完毕了 yield item meta是浅复制，必要时需要深复制。 import copy meta={'key':copy.deepcopy('value')} meta是一个dict，主要是用解析函数之间传递值，一种常见的情况：在parse中给item某些字段提取了值，但是另外一些值需要在parse_item中提取，这时候需要将parse中的item传到parse_item方法中处理，显然无法直接给parse_item设置而外参数。 Request对象接受一个meta参数，一个字典对象，同时Response对象有一个meta属性可以取到相应request传过来的meta。所以解决上述问题可以这样做： def parse(self, response): # item = ItemClass() yield Request(url, meta={'item': item}, callback=self.parse_item) def parse(self, response): item = response.meta['item'] item['field'] = value yield item Request和Response之间如何传参 有些时候需要将两个页面的内容合并到一个item里面，这时候就需要在yield scrapy.Request的同时，传递一些参数到一下页面中。这时候可以这样操作。 request=scrapy.Request(houseurl,method='GET',callback=self.showhousedetail) request.meta['biid']=biid yield request def showhousedetail(self,response): house=HouseItem() house['bulidingid']=response.meta['biid'] Response 对象 Response 对象一般是有 scrapy 自动构建的，因此不用关心如何构建，二十如何使用它，Response 有很多属性可以用来获取数据，主要有以下属性： meta ： 其他请求传过来的 meta 属性，可以保持多个请求之间的数据连接 encoding : 返回当前字符串编码和解码的格式 text 将返回的数据作为 Unicode 字符返回 body : 将返回的字符串作为 bytes 字符串返回 xpath : xpath 选择器 css : css 选择器 发送 post 请求 有时候想要请求数据的时候发送 post 请求，那么需要使用 Request 的子类 FormRequest 来实现， 如果想要爬虫一开始的时候就发送 post 请求，那么需要在爬虫类中重写 start_requests(self) 方法， 并且不在调用 start_urls 里的 url Update time： 2020-05-28 "},"Scrapy/Scrapy模拟人人网登录.html":{"url":"Scrapy/Scrapy模拟人人网登录.html","title":"Scrapy模拟人人网登录","keywords":"","body":"Scrapy模拟人人网登录 模拟人人网登录 发送 post 请求 有时候想要请求数据的时候发送 post 请求，那么需要使用 Request 的子类 FormRequest 来实现， 如果想要爬虫一开始的时候就发送 post 请求，那么需要在爬虫类中重写 start_requests(self) 方法， 并且不在调用 start_urls 里的 url # -*- coding: utf-8 -*- # renren.py import scrapy class RenrenSpider(scrapy.Spider): name = 'renren' allowed_domains = ['renren.com'] start_urls = ['http://renren.com/'] def start_requests(self): # 人人网登录的接口 url = \"http://www.renren.com/PLogin.do\" data = {\"email\": \"1315152****\", 'password': \"hu******\"} # 模拟登录 request = scrapy.FormRequest(url, formdata=data, callback=self.parse_page) yield request def parse_page(self, response): # 登录成功后访问个人主页 url = \"http://photo.renren.com/photo/972862448/albumlist/v7?offset=0&limit=40#\" respons = scrapy.Request(url, callback=self.parse_profile ) yield respons def parse_profile(self, response): # 将页面存储到本地 with open('dp.html', 'w', encoding='utf-8') as fp: fp.write(response.text) start_requests() 登录成功后 scrapy 会自动保存cookie 等信息。 Update time： 2020-05-28 "},"Scrapy/Scrapy模拟登录豆瓣网.html":{"url":"Scrapy/Scrapy模拟登录豆瓣网.html","title":"Scrapy模拟登录豆瓣网","keywords":"","body":"Scrapy模拟登录豆瓣网 # -*- coding: utf-8 -*- import scrapy from urllib import request from PIL import Image from urllib import request from base64 import b64encode import requests class DoubanSpider(scrapy.Spider): name = 'douban' allowed_domains = ['douban.com'] profile_url = 'https://www.douban.com/people/97956064/' login_url = 'https://accounts.douban.com/login' editsignature_url = 'https://www.douban.com/j/people/97956064/edit_signature' # 登录的 url start_urls = [\"https://accounts.douban.com/login\"] def parse(self, response): # 进入等率界面获取所需要的表单数据 # 登录需要的表单数据 formdata = { 'source': 'None', 'redir': 'https://www.douban.com/', 'form_email': '970138074@qq.com', 'form_password': 'pythonspider', 'remember': 'on', 'login': '登录' } # 获取验证码图片的 url captcha_url = response.css('img#captcha_image::attr(src)').get() # 判断是否需要验证码 if captcha_url: \"\"\" captcha-solution, captcha-id 都是动态变化的，需要临时获取 \"\"\" # 识别验证码 captcha = self.regonize_captcha(captcha_url) # 追加表单数据 formdata['captcha-solution'] = captcha captcha_id = response.xpath(\"//input[@name='captcha-id']/@value\").get() formdata['captcha-id'] = captcha_id yield scrapy.FormRequest(url=self.login_url, formdata=formdata, callback=self.parse_after_login) def parse_after_login(self, response): if response.url == 'https://www.douban.com/': yield scrapy.Request(self.profile_url, callback=self.parse_profile) print('登录成功！') else: print('登录失败！') def parse_profile(self, response): print(response.url) if response.url == self.profile_url: ck = response.xpath(\"//input[@name='ck']/@value\").get() formdata = { 'ck': ck, 'signature': '我可以自动识别图形验证码啦~~' } yield scrapy.FormRequest(self.editsignature_url, formdata=formdata, callback=self.parse_none) else: print('没有进入到个人中心') def parse_none(self,response): pass def regonize_captcha(self, image_url): captcha_url = image_url # 将验证码图片保存到本地 request.urlretrieve(captcha_url, 'captcha.png') # 阿里云识别验证码的 网站 recognize_url = 'http://jisuyzmsb.market.alicloudapi.com/captcha/recognize?type=e' formdata = {} with open('captcha.png', 'rb') as fp: data = fp.read() pic = b64encode(data) formdata['pic'] = pic appcode = '831a890b2cfe4ea0a8e345078434ebfc' headers = { 'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8', 'Authorization': 'APPCODE ' + appcode } # 识别的结果，json格式的文件 response = requests.post(recognize_url, data=formdata, headers=headers) result = response.json() # 获取验证码 code = result['result']['code'] return code # 认为识别验证码 # def regonize_captcha(self,image_url): # request.urlretrieve(image_url, 'captcha.png') # image = Image.open('captcha.png') # image.show() # image.close() # captcha = input('请输入验证码：') # return captcha Update time： 2020-05-28 "},"Scrapy/Settings.html":{"url":"Scrapy/Settings.html","title":"Settings","keywords":"","body":"Settings Scrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。 设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。 设定可以通过下面介绍的多种机制进行设置。 设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。 ROBOTSTXT_OBEY 设置为False。默认是True。即遵守机器协议，那么在爬虫的时候，scrapy首先去找robots.txt文件，如果没有找到。则直接停止爬取。 DEFAULT_REQUEST_HEADERS 添加User-Agent。这个也是告诉服务器，我这个请求是一个正常的请求，不是一个爬虫。 # Override the default request headers: DEFAULT_REQUEST_HEADERS = { 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Language': 'en', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36' } ITEM_PIPELINES 启用一个Item Pipeline组件 ITEM_PIPELINES = { 'myproject.pipelines.PricePipeline': 300, 'myproject.pipelines.JsonWriterPipeline': 800, } 分配给每个类的整型值，确定了他们运行的顺序，item按数字从低到高的顺序，通过pipeline，通常将这些数字定义在0-1000范围内。 DOWNLOAD_DELAY 默认: 0 下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度， 减轻服务器压力。同时也支持小数: DOWNLOAD_DELAY = 0.25 # 250 ms of delay 更多参考 Scrapy-Settings Update time： 2020-05-27 "},"Scrapy/使用Scrapy框架爬取糗事百科段子.html":{"url":"Scrapy/使用Scrapy框架爬取糗事百科段子.html","title":"使用Scrapy框架爬取糗事百科段子","keywords":"","body":"使用Scrapy框架爬取糗事百科段子 使用命令创建一个项目 scrapy startproject spider cd 到 该项目，创建一个爬虫 scrapy genspider qsbk \"www.qiushibaike.com\" 爬虫代码解析 # qsbk.py class QsbkSpider(scrapy.Spider): name = 'qsbk' allowed_domains = ['www.qiushibaike.com'] start_urls = ['https://www.qiushibaike.com/text/page/1/'] def parse(self, response): pass 要创建一个Spider，那么必须自定义一个类，继承自scrapy.Spider，然后在这个类中定义三个属性和一个方法。 name：这个爬虫的名字，名字必须是唯一的。 allow_domains：允许的域名。爬虫只会爬取这个域名下的网页，其他不是这个域名下的网页会被自动忽略。 start_urls：爬虫从这个变量中的url开始。 parse：引擎会把下载器下载回来的数据扔给爬虫解析，爬虫再把数据传给这个parse方法。这个是个固定的写法。这个方法的作用有两个，第一个是提取想要的数据。第二个是生成下一个请求的url。 将 start_urls 改为自己想要爬取的页面的url 修改settings.py代码 在做一个爬虫之前，一定要记得修改setttings.py中的设置。两个地方是强烈建议设置的。 ROBOTSTXT_OBEY设置为False。默认是True。即遵守机器协议，那么在爬虫的时候，scrapy首先去找robots.txt文件，如果没有找到。则直接停止爬取。 DEFAULT_REQUEST_HEADERS添加User-Agent。这个也是告诉服务器，我这个请求是一个正常的请求，不是一个爬虫。 完成的爬虫代码 爬虫部分代码： # -*- coding: utf-8 -*- import scrapy # 从根目录 导入 item 类 from spider.items import SpiderItem class QsbkSpider(scrapy.Spider): name = 'qsbk' allowed_domains = ['www.qiushibaike.com'] start_urls = ['https://www.qiushibaike.com/text/page/1/'] def parse(self, response): contents = response.xpath('//div[@class=\"article block untagged mb15 typs_hot\"]') print(\"*\" * 30) for duanzi in contents: # get() 获取一个内容 author = duanzi.xpath(\".//h2/text()\").get().strip() # getall() 获取所有内容，返回一个列表 content = duanzi.xpath('.//div[@class=\"content\"]//text()').getall() content = ' '.join(content).strip() # 另外一种传递数据的方式 创建一个 SpiderItem 对象 item = SpiderItem(author=author, content=content) ''' 或者使用这种方式 item = SpiderItem() item['author'] = author item['content'] = content ''' yield item # 将数据传到pipelines response 对象可以直执行 xpath语法来提取数据，如果想要获取里面的字符串，那么应该执行getall()或者get() 方法, getall() 方法获取所有文本，返回的是一个列表 get() 方法获取第一个，返回的是一个 str 类型 如果数据解析回来，要传给 pipeline 处理，那么可以使用 yield 来返回，或者收集所有的最后使用一个 return 来返回, items.py部分代码： import scrapy class SpiderItem(scrapy.Item): author = scrapy.Field() content = scrapy.Field() item 建议在“items.py” 中定义好， pipeline部分代码： import json class SpiderPipeline: def __init__(self): # 打开文件 或者在open_spider() 中打开文件 self.fp = open('duanzi.json','w',encoding='utf8') def open_spider(self,spider): pass def process_item(self, item, spider): # 采用 item 的方式 需要将其转化为字典 item_json = json.dumps(dict(item), ensure_ascii=False) self.fp.write(item_json +'\\n') return item def close_spider(self,spider): # 关闭文件 self.fp.close() 其中 def process_item(self, item, spider) 方法是必须有的，别的虽需要可以自己添加 优化 json 数据存储方式 (一) 导入 from scrapy.exporters import JsonItemExporter from scrapy.exporters import JsonItemExporter class SpiderPipeline: def __init__(self): # 以 wb 的方式打开文件 self.fp = open('duanzi.json', 'wb') # 创建对象 self.exporter = JsonItemExporter(self.fp, ensure_ascii=False, encoding='utf8') # 开始 self.exporter.start_exporting() def open_spider(self, spider): pass def process_item(self, item, spider): # 采用 item 的方式 需要将其转化为字典 #item_json = json.dumps(dict(item), ensure_ascii=False) # self.fp.write(item_json + '\\n') # 不再需要将 item 转化为字典 self.exporter.export_item(item) return item def close_spider(self, spider): self.exporter.finish_exporting() # 关闭文件 self.fp.close() 存储的结果 为一个列表，每个字典是列表的一项，当数据过大时，不推荐这种方式，因为其将整个 item z作为一项 导入文件的。 优化 json 数据存储方式 (二) 导入 from scrapy.exporters import JsonLinesItemExporter from scrapy.exporters import JsonLinesItemExporter class SpiderPipeline: def __init__(self): # 以 wb 的方式打开文件 self.fp = open('duanzi.json', 'wb') # 创建对象 self.exporter = JsonLinesItemExporter(self.fp, ensure_ascii=False, encoding='utf8') def open_spider(self, spider): pass def process_item(self, item, spider): self.exporter.export_item(item) return item def close_spider(self, spider): # 关闭文件 self.fp.close() 不需要开启和关闭，导入数据后的文件，仍然时一个字典一行 JsonItemExporter 和 JsonLinesItemExporter 保存数据的时候使用这两个类，让操作变的更简单 JsonItemExporter 每次将数据添加到内存中，最后统一写入到磁盘，好处时，存储的数据是一个满足 json 规则的数据，坏处是如果数据量比较大，那么内存消耗严重 JsonLinesItemExporter 每次调用 export_item 的时候就把这个item 存储到硬盘中，坏处是每一个字典是一行，整个文件不是满足json格式的文件，好处是每次处理数据的时候直接存储到硬盘，这样不会耗内存，数据也比较安全。 Update time： 2020-05-27 "},"Scrapy/糗事百科之抓取多个页面.html":{"url":"Scrapy/糗事百科之抓取多个页面.html","title":"糗事百科之抓取多个页面","keywords":"","body":"糗事百科之抓取多个页面 修改之前的 qsbk.py 文件 # -*- coding: utf-8 -*- import scrapy # 从根目录 导入 item 类 from spider.items import SpiderItem class QsbkSpider(scrapy.Spider): name = 'qsbk' allowed_domains = ['www.qiushibaike.com'] start_urls = ['https://www.qiushibaike.com/text/page/1/'] base_domain = \"https://www.qiushibaike.com\" def parse(self, response): contents = response.xpath('//div[@class=\"col1 old-style-col1\"]/div') for duanzi in contents: author = duanzi.xpath(\".//h2/text()\").get().strip() content = duanzi.xpath('.//div[@class=\"content\"]//text()').getall() content = ' '.join(content).strip() item = SpiderItem(author=author, content=content) yield item # 查找下页的 url # 查找最后一个 li 标签的 href 属性， # 若为空，表示当前页为最后一页，返回 # 否在, 请求下页的 url 获取内容 next_url = response.xpath('//div[@class=\"col1 old-style-col1\"]' '/ul/li[last()]/a/@href').get() if not next_url: return else: yield scrapy.Request(self.base_domain + next_url, callback=self.parse) 查找下页的 url 利用 scrapy.Request() 请求下一页，并利用回调函数进行解析html Update time： 2020-05-27 "},"Scrapy/CrawlSpider爬虫.html":{"url":"Scrapy/CrawlSpider爬虫.html","title":"CrawlSpider爬虫","keywords":"","body":"CrawlSpider爬虫 在上一个糗事百科的爬虫案例中。我们是自己在解析完整个页面后获取下一页的url，然后重新发送一个请求。有时候我们想要这样做，只要满足某个条件的 url，都给我进行爬取。那么这时候我们就可以通过CrawlSpider 来帮我们完成了。CrawlSpider 继承自 Spider，只不过是在之前的基础之上增加了新的功能，可以定义爬取的url的规则，以后scrapy碰到满足条件的url都进行爬取，而不用手动的yield Request。 创建CrawlSpider爬虫： 之前创建爬虫的方式是通过scrapy genspider [爬虫名字] [域名]的方式创建的。如果想要创建CrawlSpider爬虫，那么应该通过以下命令创建： scrapy genspider -t crawl [爬虫名字] [域名] scrapy genspider -t crawl wxapp 'wxapp-union.com' LinkExtractors链接提取器： 使用LinkExtractors可以不用程序员自己提取想要的url，然后发送请求。这些工作都可以交给LinkExtractors，他会在所有爬的页面中找到满足规则的url，实现自动的爬取。 class scrapy.linkextractors.LinkExtractor( allow = (), deny = (), allow_domains = (), deny_domains = (), deny_extensions = None, restrict_xpaths = (), tags = ('a','area'), attrs = ('href'), canonicalize = True, unique = True, process_value = None ) 主要参数讲解： allow：允许的url。所有满足这个正则表达式的url都会被提取。 deny：禁止的url。所有满足这个正则表达式的url都不会被提取。 allow_domains：允许的域名。只有在这个里面指定的域名的url才会被提取。 deny_domains：禁止的域名。所有在这个里面指定的域名的url都不会被提取。 restrict_xpaths：严格的xpath。和allow共同过滤链接。 Rule规则类： 定义爬虫的规则类。 class scrapy.spiders.Rule( link_extractor, callback = None, cb_kwargs = None, follow = None, process_links = None, process_request = None ) 主要参数讲解： link_extractor：一个LinkExtractor对象，用于定义爬取规则。 callback：满足这个规则的url，应该要执行哪个回调函数。因为CrawlSpider使用了parse作为回调函数，因此不要覆盖parse作为回调函数自己的回调函数。 follow：指定根据该规则从response中提取的链接是否需要跟进。 process_links：从link_extractor中获取到链接后会传递给这个函数，用来过滤不需要爬取的链接。 微信小程序社区CrawlSpider案例 初始的 爬虫文件 import scrapy from scrapy.linkextractors import LinkExtractor from scrapy.spiders import CrawlSpider, Rule class WxappSpiderSpider(CrawlSpider): name = 'wxapp_spider' allowed_domains = ['wxapp-union.com'] start_urls = [\"http://'wxapp-union.com'/\"] rules = ( Rule(LinkExtractor(allow=r'Items/'), callback='parse_item', follow=True), ) def parse_item(self, response): item = {} #item['domain_id'] = response.xpath('//input[@id=\"sid\"]/@value').get() #item['name'] = response.xpath('//div[@id=\"name\"]').get() #item['description'] = response.xpath('//div[@id=\"description\"]').get() return item 分析每页(教程栏)的url http://www.wxapp-union.com/portal.php?mod=list&catid=2&page=1 http://www.wxapp-union.com/portal.php?mod=list&catid=2&page=2 对于每一页 只有最后面的数字不同 分析每个详情页的 url http://www.wxapp-union.com/article-5985-1.html http://www.wxapp-union.com/article-6015-1.html http://www.wxapp-union.com/article-6002-1.html 只有中间的四个数字不同， 修改 rules rules = ( # 匹配每页的url Rule(LinkExtractor(allow=r'.+mod=list&catid=2&page=\\d'), # 允许的匹配规则，可以是正则匹配 follow=True), # 匹配详情页的 url Rule(LinkExtractor(allow=r'.+/article-.+\\.html'), callback=\"parse_detail\", # 解析详情页的回调函数，字符串格式的 follow=False # 详情页不跟进 ) ) CrawlSpider 需要使用 LinkExtractor 和Rule 决定爬虫的具体走向 allow 设置规则的方法：能够限制我们想得到的 url ， 不要根其他的 url 产生相同的正则表达式即可 什么情况下使用 follow : 如果在爬取页面的时候，需要满足当前的 url 再进行跟进，那么就设置为 True，否在设置为 False 什么情况下指定 callback ： 如果这个 url 对应的页面只是为了获取更多的 url, 并不需要页面的数据，那么可以不指定 callback, 如果想要获取 url 对应页面中的数据，那么就需要指定 callback 其他相关文件的设置和Spider 一样 Update time： 2020-05-28 "},"案例/":{"url":"案例/","title":"案例","keywords":"","body":"案例 Update time： 2020-07-19 "},"案例/爬取地理坐标.html":{"url":"案例/爬取地理坐标.html","title":"爬取地理坐标","keywords":"","body":"爬取地理坐标 爬取GPSspg查询网 import requests, re from urllib import parse def query(region): header = {'User-Agent': 'Opera/8.0 (Windows NT 5.1; U; en)'} url = 'http://apis.map.qq.com/jsapi?' data = { 'qt': 'poi', 'wd': region, 'pn': 0, 'rn': 10, 'rich_source': 'qipao', 'rich': 'web', 'nj': 0, 'c': 1, 'key': 'FBOBZ-VODWU-C7SVF-B2BDI-UK3JE-YBFUS', 'output': 'jsonp', 'pf': 'jsapi', 'ref': 'jsapi', 'cb': 'qq.maps._svcb3.search_service_0'} coordinate_url = url + parse.urlencode(data) r = requests.get(coordinate_url, headers=header) longitude = re.findall('\"pointx\":\\s*\"(.+?)\"', r.text)[0] latitude = re.findall('\"pointy\":\\s*\"(.+?)\"', r.text)[0] print([region, longitude, latitude]) query('佛山南海') # [‘佛山南海’, ‘113.142780’, ‘23.028820’] 爬取百度地图拾取坐标系统 from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC import re, pandas as pd def coordinate(site): # 创建浏览器驱动对象 driver = webdriver.Firefox() driver.get('http://api.map.baidu.com/lbsapi/getpoint/index.html') # 显式等待，设置timeout wait = WebDriverWait(driver, 9) # 判断输入框是否加载 input = wait.until( EC.presence_of_element_located( (By.CSS_SELECTOR, '#localvalue'))) # 判断搜索按钮是否加载 submit = wait.until( EC.element_to_be_clickable( (By.CSS_SELECTOR, '#localsearch'))) # 输入搜索词，点击搜索按钮 input.clear() input.send_keys(site) submit.click() # 等待坐标 wait.until( EC.presence_of_element_located( (By.CSS_SELECTOR, '#no_0'))) # 获取网页文本，提取经纬度 source = driver.page_source xy = re.findall('坐标：([\\d.]+),([\\d.]+)', source) # 转浮点数，取中位数 df = pd.DataFrame(xy, columns=['longitude', 'latitude']) df['longitude'] = pd.to_numeric(df['longitude']) df['latitude'] = pd.to_numeric(df['latitude']) longitude = df['longitude'].median() latitude = df['latitude'].median() # 关闭浏览器驱动 driver.close() return [longitude, latitude] print(coordinate('南海桂城地铁站')) # [113.1611575, 23.044811000000003] 在使用 webdriver 的时候，需要将下载的驱动器加载到 PATH 环境变量中，另一种方案：将驱动器放到当前脚本目录下。 参考 转自Python 爬取经纬度 Python 爬取经纬度 获取GPS / 经纬度转换 百度地图：拾取坐标系统 Update time： 2020-07-19 "}}